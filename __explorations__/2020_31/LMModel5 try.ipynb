{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *\n",
    "\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "\n",
    "Path.BASE_PATH = path\n",
    "\n",
    "lines = L()\n",
    "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
    "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
    "\n",
    "\n",
    "text = ' . '.join([l.strip() for l in lines])\n",
    "tokens = text.split(' ')\n",
    "tokens = text.split(' ')\n",
    "vocab = L(*tokens).unique()\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "nums = L(word2idx[i] for i in tokens)\n",
    "\n",
    "\n",
    "##################################\n",
    "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n",
    "bs = 64\n",
    "cut = int(len(seqs) * 0.8)\n",
    "#d = default_device()\n",
    "#print(f\"device is {d}\") # xm.xla_device()\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False,\n",
    "                            # device=d,\n",
    "                             drop_last=True\n",
    "                            )\n",
    "dls.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *\n",
    "\n",
    "class LMModel5(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        print(\"LMModel5 init\")\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
    "        print(\"LMModel5 end init\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"LMModel5 forward start\")\n",
    "        res,h = self.rnn(self.i_h(x), self.h)\n",
    "        print(\"LMModel5 forward detaching\")\n",
    "        self.h = h.detach()\n",
    "        print(\"LMModel5 forward calculating output\")\n",
    "        ooo = self.h_o(res)\n",
    "        print(\"LMModel5 forward returning output\")\n",
    "        return ooo\n",
    "    \n",
    "    def reset(self):\n",
    "      print(\"LMModel5 init reset\")\n",
    "      self.h.zero_()\n",
    "      print(\"LMModel5 end reset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from fastai2.test_utils import VerboseCallback\n",
    "import pdb\n",
    "\n",
    "class CountBatches(Callback):\n",
    "  def begin_fit(self):\n",
    "    print(f\"begin_fit lenghts => {len(self.learn.dls.train)}, {len(self.learn.dls.valid)}\")\n",
    "  def begin_batch(self):\n",
    "    print(f\"b_b={self.learn.iter}\")\n",
    "\n",
    "@patch_to(BaseLoss)\n",
    "def __call__(self, inp, targ, **kwargs):\n",
    "        inp  = inp .transpose(self.axis,-1).contiguous()\n",
    "        targ = targ.transpose(self.axis,-1).contiguous()\n",
    "        if self.floatify and targ.dtype!=torch.float16: targ = targ.float()\n",
    "        if targ.dtype in [torch.int8, torch.int16, torch.int32]: targ = targ.long()\n",
    "        if self.flatten: inp = inp.view(-1,inp.shape[-1]) if self.is_2d else inp.view(-1)\n",
    "        pdb.set_trace()\n",
    "        return self.func.__call__(inp, targ.view(-1) if self.flatten else targ, **kwargs)\n",
    "\n",
    "def loss_func(inp, targ):\n",
    "    print(\"loss_func\", inp.shape, targ.shape, \"loss_func\")\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))\n",
    "@log_args(but='cbs')\n",
    "@patch_to(Learner)\n",
    "def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False):\n",
    "    print(\"==================================== add callbacks!!!!\")\n",
    "    with self.added_cbs(cbs):\n",
    "            if reset_opt or not self.opt: self.create_opt()\n",
    "            if wd is None: wd = self.wd\n",
    "            if wd is not None: self.opt.set_hypers(wd=wd)\n",
    "            self.opt.set_hypers(lr=self.lr if lr is None else lr)\n",
    "\n",
    "            try:\n",
    "                self._do_begin_fit(n_epoch)\n",
    "                for epoch in range(n_epoch):\n",
    "                    try:\n",
    "                        print(f\"doing epoch {self.epoch} of {len(self.dls.train)} and {len(self.dls.valid)}\")\n",
    "                        self.epoch=epoch;          self('begin_epoch')\n",
    "                        print(\"-> self._do_epoch_train()\")\n",
    "                        self._do_epoch_train()\n",
    "                        print(\"<- self._do_epoch_train()\")\n",
    "                        print(\"---> self._do_epoch_validate()\")\n",
    "                        self._do_epoch_validate()\n",
    "                        print(\"<--- self._do_epoch_validate()\")\n",
    "                    except CancelEpochException:   self('after_cancel_epoch')\n",
    "                    finally:                       self('after_epoch')\n",
    "\n",
    "            except CancelFitException:             self('after_cancel_fit')\n",
    "            finally:\n",
    "                print(f\"+++++++++++++++++++++++ WILL CALL AFTER FIT {self}\")\n",
    "                self('after_fit')\n",
    "                print(\"CALLED!!!! AFTER FIT\")\n",
    "                self._end_cleanup()\n",
    "                print(\"************* ###### CALLED!!!! cleanup\")\n",
    "    print(\"!!!!!!!!!!!!!!! ! ! ! ! !!!!!!!!!!!!! ENDED with self.callbacks....\")\n",
    "    pdb.set_trace()\n",
    "\n",
    "@patch_to(Learner)\n",
    "def all_batches(self):\n",
    "        self.n_iter = len(self.dl)\n",
    "        print(f\"????????????????????????? self.n_iter={self.n_iter}\")\n",
    "        for o in enumerate(self.dl):\n",
    "          print(\"??????????????????????????? self.one_batch\")\n",
    "          self.one_batch(*o)\n",
    "          print(\"??????????????????????????? self.one_batch did end!!!\")\n",
    "@patch_to(Learner)\n",
    "def one_batch(self, i, b):\n",
    "        self.iter = i\n",
    "        try:\n",
    "            print(\"--------------> --------------> --------------> ### 0\")\n",
    "            self._split(b);                                  self('begin_batch')\n",
    "            print(\"--------------> --------------> --------------> ### 1\")\n",
    "            self.pred = self.model(*self.xb);                self('after_pred')\n",
    "            print(\"--------------> --------------> --------------> ### 2\")\n",
    "            if len(self.yb) == 0: return\n",
    "            print(f\"--------------> --------------> --------------> ### 3 {self.pred.shape} and lenth ={len(self.yb)} yb_0={self.yb[0].shape}\")\n",
    "            self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')\n",
    "            print(\"--------------> --------------> --------------> ### 4\")\n",
    "            if not self.training: return\n",
    "            print(\"--------------> --------------> --------------> ### 5\")\n",
    "            print(\"############ --------------> --------------> -------------->\")\n",
    "            self.loss.backward();                            self('after_backward')\n",
    "            print(\"############ <-------------- <-------------- <--------------\")\n",
    "            print(\"--------------> --------------> --------------> ### 6\")\n",
    "            self.opt.step();                                 self('after_step')\n",
    "            print(\"--------------> --------------> --------------> ### 7\")\n",
    "            self.opt.zero_grad()\n",
    "            print(\"--------------> --------------> --------------> ### 8.0\")\n",
    "        except CancelBatchException:                         self('after_cancel_batch')\n",
    "        finally:\n",
    "          print(\"--------------> --------------> --------------> ### 9999999\")\n",
    "          print(\"¿¿¿¿¿¿¿¿¿¿¿¿¿¿will call after batch!!!\")\n",
    "          self('after_batch')\n",
    "          print(\"¿¿¿¿¿¿¿¿¿¿¿¿¿¿      called after batch!!!\")\n",
    "\n",
    "learn = Learner(dls, LMModel5(len(vocab), 64, 1), \n",
    "                #loss_func=CrossEntropyLossFlat(), \n",
    "                loss_func=loss_func,\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(1, 3e-3, cbs=[CountBatches(), VerboseCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = L()\n",
    "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
    "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' . '.join([l.strip() for l in lines])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = L(*tokens).unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "nums = L(word2idx[i] for i in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs = 64\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel5(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = h.detach()\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self): self.h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, LMModel5(len(vocab), 64, 2), \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try again again again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "\n",
    "#hide\n",
    "Path.BASE_PATH = path\n",
    "\n",
    "lines = L()\n",
    "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
    "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
    "    \n",
    "text = ' . '.join([l.strip() for l in lines])\n",
    "\n",
    "tokens = text.split(' ')\n",
    "\n",
    "vocab = L(*tokens).unique()\n",
    "\n",
    "\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "nums = L(word2idx[i] for i in tokens)\n",
    "\n",
    "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n",
    "bs = 64\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you dont execute this two you get the `ValueError: Expected input batch_size (192) to match target batch_size (64).` So lets execute them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 16\n",
    "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
    "         for i in range(0,len(nums)-sl-1,sl))\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
    "                             group_chunks(seqs[cut:], bs),\n",
    "                             bs=bs, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel5(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = h.detach()\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self): self.h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.056531</td>\n",
       "      <td>2.651854</td>\n",
       "      <td>0.461263</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.138902</td>\n",
       "      <td>1.819939</td>\n",
       "      <td>0.468831</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.696971</td>\n",
       "      <td>1.985687</td>\n",
       "      <td>0.324137</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.477333</td>\n",
       "      <td>1.823898</td>\n",
       "      <td>0.450765</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.299323</td>\n",
       "      <td>1.970886</td>\n",
       "      <td>0.482747</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.125414</td>\n",
       "      <td>2.154227</td>\n",
       "      <td>0.499756</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.968370</td>\n",
       "      <td>2.304337</td>\n",
       "      <td>0.518717</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.839025</td>\n",
       "      <td>2.417535</td>\n",
       "      <td>0.521484</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.727712</td>\n",
       "      <td>2.488352</td>\n",
       "      <td>0.530111</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.632702</td>\n",
       "      <td>2.518758</td>\n",
       "      <td>0.532878</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.557515</td>\n",
       "      <td>2.526410</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.502025</td>\n",
       "      <td>2.582220</td>\n",
       "      <td>0.541016</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.464866</td>\n",
       "      <td>2.552371</td>\n",
       "      <td>0.552734</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.442643</td>\n",
       "      <td>2.564526</td>\n",
       "      <td>0.553060</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.431220</td>\n",
       "      <td>2.569933</td>\n",
       "      <td>0.552572</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel5(len(vocab), 64, 2), \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.forget_gate = nn.Linear(ni + nh, nh)\n",
    "        self.input_gate  = nn.Linear(ni + nh, nh)\n",
    "        self.cell_gate   = nn.Linear(ni + nh, nh)\n",
    "        self.output_gate = nn.Linear(ni + nh, nh)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        h,c = state\n",
    "        h = torch.stack([h, input], dim=1)\n",
    "        forget = torch.sigmoid(self.forget_gate(h))\n",
    "        c = c * forget\n",
    "        inp = torch.sigmoid(self.input_gate(h))\n",
    "        cell = torch.tanh(self.cell_gate(h))\n",
    "        c = c + inp * cell\n",
    "        out = torch.sigmoid(self.output_gate(h))\n",
    "        h = outgate * torch.tanh(c)\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.ih = nn.Linear(ni,4*nh)\n",
    "        self.hh = nn.Linear(nh,4*nh)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        h,c = state\n",
    "        # One big multiplication for all the gates is better than 4 smaller ones\n",
    "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n",
    "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
    "        cellgate = gates[3].tanh()\n",
    "\n",
    "        c = (forgetgate*c) + (ingate*cellgate)\n",
    "        h = outgate * c.tanh()\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel6(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.015898</td>\n",
       "      <td>2.699370</td>\n",
       "      <td>0.384196</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.129531</td>\n",
       "      <td>1.990293</td>\n",
       "      <td>0.290690</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.590543</td>\n",
       "      <td>1.769736</td>\n",
       "      <td>0.498617</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.301828</td>\n",
       "      <td>2.329215</td>\n",
       "      <td>0.476888</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.080614</td>\n",
       "      <td>2.432390</td>\n",
       "      <td>0.515951</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.864964</td>\n",
       "      <td>1.884870</td>\n",
       "      <td>0.606201</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.613487</td>\n",
       "      <td>1.557686</td>\n",
       "      <td>0.649984</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.380154</td>\n",
       "      <td>1.564096</td>\n",
       "      <td>0.666992</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.224778</td>\n",
       "      <td>1.320145</td>\n",
       "      <td>0.710612</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.134050</td>\n",
       "      <td>1.333302</td>\n",
       "      <td>0.726888</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.080824</td>\n",
       "      <td>1.337515</td>\n",
       "      <td>0.739339</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.051982</td>\n",
       "      <td>1.337239</td>\n",
       "      <td>0.737630</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.037146</td>\n",
       "      <td>1.329395</td>\n",
       "      <td>0.737874</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.029226</td>\n",
       "      <td>1.331835</td>\n",
       "      <td>0.740560</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>1.336482</td>\n",
       "      <td>0.739176</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel6(len(vocab), 64, 2), \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p): self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        mask = x.new(*x.shape).bernoulli_(1-p)\n",
    "        return x * mask.div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel7(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h_o.weight = self.i_h.weight\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raw,h = self.rnn(self.i_h(x), self.h)\n",
    "        out = self.drop(raw)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(out),raw,out\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_x = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n",
    "                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n",
    "                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.703845</td>\n",
       "      <td>2.301995</td>\n",
       "      <td>0.435547</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.975745</td>\n",
       "      <td>1.857219</td>\n",
       "      <td>0.524821</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.307809</td>\n",
       "      <td>0.923792</td>\n",
       "      <td>0.724202</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.759294</td>\n",
       "      <td>0.828541</td>\n",
       "      <td>0.744873</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.455115</td>\n",
       "      <td>0.607791</td>\n",
       "      <td>0.822510</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.304398</td>\n",
       "      <td>0.467311</td>\n",
       "      <td>0.857341</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.227844</td>\n",
       "      <td>0.439848</td>\n",
       "      <td>0.865153</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.391412</td>\n",
       "      <td>0.870036</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.160428</td>\n",
       "      <td>0.366598</td>\n",
       "      <td>0.879232</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.143744</td>\n",
       "      <td>0.340913</td>\n",
       "      <td>0.897542</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.131981</td>\n",
       "      <td>0.444382</td>\n",
       "      <td>0.850260</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.122374</td>\n",
       "      <td>0.367438</td>\n",
       "      <td>0.879964</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.116014</td>\n",
       "      <td>0.388417</td>\n",
       "      <td>0.876546</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.111177</td>\n",
       "      <td>0.365198</td>\n",
       "      <td>0.882161</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.108672</td>\n",
       "      <td>0.369336</td>\n",
       "      <td>0.880615</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_x.fit_one_cycle(15, 1e-2, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
