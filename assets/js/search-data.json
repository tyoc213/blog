{
  
    
        "post0": {
            "title": "Finding nemo a bug journey",
            "content": "What happened at first . The major part of the last months we were trying to solve one bug that we didn&#39;t know how to solve until this days, some time before fastai version 2 release having a POC and passing the hackathon we participated we left a little the project, but when we came back to it, there were some extrange things happening, some models where not training and apparently others where training, we didn&#39;t understand what happened, in that time we thought we introduced an error in our code some way. . The journey . So, these last few days since I had locally installed XLA and been able to run things, I planned to take a new round to find our bug and it was a perfect opportunity to test what having locally installed pytorch+xla+fatai_xla_etensions could do. . So I passed a lot of assumptions to finally find the solution. . So after having locally installed XLA, one of the first things I wanted to test is if locally I could reproduce the error, and I could! | The first one was that the error was in our code, but having locally XLA allowed me to test the exact same code changing the device to either: CPU, CUDA or XLA. So I ended up having 2 data loaders and 2 trains on the same python file. Also one main point was that I wrapped a Adam but from pytorch with OptimWrapper and it trained correctly so I was more suspicious of differences between the optimizer from fastai and the native to pytorch because there is one know difference about __getstate__ that is also a requirement for TPU Pods. | In the past we have also thought that it was a freeze unfreeze problem, but it was also discarded, so this time I was checking the optimizer, but could not find why the params were not training even when looking under the lens. | But after more testings and so on, I see that the second example started to train correctly while the first on the file not, and it was that with all the fresh runs, so I thought it was a problem with the learner, but could not find a &quot;real problem&quot; so I returned back to the optimizer and all, but this time I have a new &quot;tool&quot; I learned, counting the trainable parameters so, the trainable parameters their gradients are updated when you call backward, so I started to count there and for the first example they where always zero while for the second run since start, they have a number. So the next task was to find why on the first the trainable parameters are always zero and the second not. | But I still didn&#39;t get why one model was training and the other one not! . I passed _BaseOptimizer, Optimizer, Learner and others and still could not find the problem, so I decided to compare models and found the problem! I updated the example I found in pytorch forums https://discuss.pytorch.org/t/two-models-with-same-weights-different-results/8918/7 the original one at first run did break because it compared tensors not on same device, so it threw error, I modified it so that it prints them nicely instead of being caught in that error. . def compare_models(model_1, model_2): models_differ = 0 for key_item_1, key_item_2 in zip(model_1.state_dict().items(), model_2.state_dict().items()): if key_item_1[1].device == key_item_2[1].device and torch.equal(key_item_1[1], key_item_2[1]): pass else: models_differ += 1 if (key_item_1[0] == key_item_2[0]): _device = f&#39;device {key_item_1[1].device}, {key_item_2[1].device}&#39; if key_item_1[1].device != key_item_2[1].device else &#39;&#39; print(f&#39;Mismatch {_device} found at&#39;, key_item_1[0]) else: raise Exception if models_differ == 0: print(&#39;Models match perfectly! :)&#39;) . And that was the solution to the problem, I focused on seeing why the models parameters were on different devices. At the end I have something like (remember I don&#39;t need to patch the optimizer because I have all installed locally). . def create_opt(self): print(&#39;trainable count before&#39;, len(self.all_params(with_grad=True))) self.opt = self.opt_func(self.splitter(self.model), lr=self.lr) print(&#39;trainable count after&#39;, len(self.all_params(with_grad=True))) if not self.wd_bn_bias: for p in self._bn_bias_state(True ): p[&#39;do_wd&#39;] = False if self.train_bn: for p in self._bn_bias_state(False): p[&#39;force_train&#39;] = True . and . print(&#39;trainable count before backward&#39;, len(self.all_params(with_grad=True))) self(&#39;before_backward&#39;) print(&#39;trainable count before backward&#39;, len(self.all_params(with_grad=True))) self._backward() self(&#39;after_backward&#39;) . So at the end I see that even that the model is moved later to the device, the first time when splitter=trainable_params in self.opt = self.opt_func(self.splitter(self.model), lr=self.lr) inside create_opt it is not there, so the parameters where stuck on CPU while the the data and the model is later moved to the XLA device. . This does not affects GPUs, but thinking about it could mean also something about the pickable behaviour of xla tensors, specially the optimizer, but that is an history for another time, right now, we have again a simple lib that works for Single-device TPUs where you need to modify zero code from fastai. . Conclusion . So the model need to be in TPU/XLA device before their parameters are taked by splitter on Optimizer initialization, I guess we assumed some things in between then and now. At the end it was not exactly an error but was. But sure it was difficult to track, now knowing what it is is solved and we can continue forward. . I hope to add in the next release a show_lowering_ops (or similar) to print the counters if you have hit some of those and it is easy to print in a model that runs with this activated. The MNIST demo should be working again don&#39;t forget to peek at fastai_xla_extensions. . EXTRA NOTE: But there was error because XLA model on CPU not trained when updating backward from data operations on XLA device? well, now I think that XLA worked on TPU with model and data copied to TPU on first time but somehow our model got stuck on CPU so not trained, it became another model separate from the execution happening on TPU (I can think of pickable things, but that is unknown at the moment) .",
            "url": "https://tyoc213.github.io/blog/xla/fastai/2020/12/12/finding-nemo-a-bug-journey.html",
            "relUrl": "/xla/fastai/2020/12/12/finding-nemo-a-bug-journey.html",
            "date": " • Dec 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Compiling xla locally",
            "content": "Introduction . Since the time I found some issues mentioning GPU support https://github.com/pytorch/xla/ I was wondering when I could use it locally because a little group at fastai community have been trying to give support to fastai and hopefully being able to run locally would be useful for that end. . Running GPU support with a docker image . So the first thing was to run it with the docker image which if you have installed container-toolkit/install-guide and then running something like docker run --gpus all -it --shm-size 16G gcr.io/tpu-pytorch/xla@sha256:efe47b7a3875ddfa3ea9c68a12ed5517c19cbb3cf272776fba64bec8a683299f or the sha or nightly you want to run like gcr.io/tpu-pytorch/xla@nightly. . After I ran that, I compiled the source as xla-instructions inside that image and after some hours I could see a &quot;hello&quot; I have made on a cpp file wonderful!!!. But it seems that all that work would be lost on the next startup, so after watch building it and ran successfully I decided to give a go into building on my own computer (if you still need that then maybe check docker commit xxxxx and docker checkpoint). . Compiling locally . To compile locally I have lurked and tested different ways, first time I build it was just with CPU support which I didn&#39;t notice (some env vars where missing), so long history short I have made a new environment with conda like conda create -n xla python=3.6 and worked inside this env. . Installing needed things . Probably I miss something, but I have to install . Don&#39;t use the cuda from apt, use directly from nvidia and install only sdk with sudo sh PATH_CUDA_DRIVERS --silent --toolkit it will be installed to /usr/local/cuda which is where it should be located (if you let Ubuntu handle installation of drivers this --toolkit will not erase that and only install sdk so when updating kernel no need to reinstall). | Install cuddn from NVIDIA from zip file and copy all h files and libs sudo cp cuda/include/cudnn.h /usr/local/cuda/include sudo cp cuda/include/cudnn.h /usr/local/cuda/include sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64 sudo cp cuda/include/cudnn_version.h /usr/local/cuda/include sudo cp cuda/include/cudnn_backend.h /usr/local/cuda/include sudo cp cuda/include/cudnn_adv_infer.h /usr/local/cuda/include sudo cp cuda/include/cudnn_adv_train.h /usr/local/cuda/include sudo cp cuda/include/cudnn_cnn_infer.h /usr/local/cuda/include sudo cp cuda/include/cudnn_cnn_train.h /usr/local/cuda/include sudo cp cuda/include/cudnn_ops_infer.h /usr/local/cuda/include sudo cp cuda/include/cudnn_ops_train.h /usr/local/cuda/include sudo cp cuda/include/cudnn.h /usr/local/cuda/include . | Install sudo apt-get install cmake | Install go to install go get github.com/bazelbuild/bazelisk and then make if you cant run bazel from command line make a ln -s /home/tyoc213/go/bin/bazelisk /home/tyoc213/go/bin/bazel because bazel is needed in the path. | sudo apt-get install clang-8 clang++-8 | pip install lark-parser | conda install -c pytorch magma-cuda110 In my case I have CUDA Version: 11.0 so I used 110 | . Get the sources . git clone --recursive https://github.com/pytorch/pytorch cd pytorch/ git clone --recursive https://github.com/pytorch/xla.git cd xla xla/scripts/apply_patches.sh . The last lines apply xla needed patches. Now you are ready to compile, but wait!!! what is missing is all the configuration that lets you build inside the docker container! . Environment vars . Which are the things I fighted most: . export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&quot;$(dirname $(which conda))/../&quot;} export TF_CUDA_COMPUTE_CAPABILITIES=&quot;7.0,7.5&quot; export CXX_ABI=0 export cxx_abi=0 export GPU_NUM_DEVICES=1 export cuda=1 # new export USE_CUDA=1 export XLA_CUDA=1 export XLA_DEBUG=1 export XLA_BAZEL_VERBOSE=0 export CXX=clang++-8 export CC=clang-8 export GLIBCXX_USE_CXX11_ABI=0 export CFLAGS=&quot;${CFLAGS} -D_GLIBCXX_USE_CXX11_ABI=0&quot; export CXXFLAGS=&quot;${CXXFLAGS} -D_GLIBCXX_USE_CXX11_ABI=0&quot; export PATH=/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/home/tyoc213/go/bin:$PATH . Probably some of them are not needed, but this worked out. Also this is not all that is needed, there is one extra set of commands needed because if not the lib will mix CXX11_ABI so it will not link (and you will not know after hours). To apply this inside the pytorch directory: . sed -i &#39;/include(CMakeDependentOption)/i set(GLIBCXX_USE_CXX11_ABI 0)&#39; CMakeLists.txt sed -i &#39;s/set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -std=c++11 -fPIC&quot;)/set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -std=c++11 -fPIC -D_GLIBCXX_USE_CXX11_ABI=0&quot;)/g&#39; third_party/gloo/CMakeLists.txt sed -i &#39;/gloo_list_append_if_unique(CUDA_NVCC_FLAGS &quot;-Xcompiler&quot; &quot;-fPIC&quot;)/i gloo_list_append_if_unique(CUDA_NVCC_FLAGS &quot;-Xcompiler&quot; &quot;-D_GLIBCXX_USE_CXX11_ABI=0&quot;)&#39; third_party/gloo/cmake/Cuda.cmake . Building . So that is all needed if I didn&#39;t miss something. So now we are ready to build this this, start at the top level pytorch: . (xla) tyoc213@u:~/Documents/github/pytorch$ python setup.py install (xla) tyoc213@u:~/Documents/github/pytorch$ cd xla (xla) tyoc213@u:~/Documents/github/pytorch/xla$ python setup.py install . In my 2015 CPU Intel(R) Core(TM) i5-6500 CPU @ 3.20GHz it taked like 2-4 hours compiling pytorch and then 8-10 hours compiling xla (which compiles internally TF). . Finally running . SO now that you have a working xla locally, you need to setup some extra vars to configure XLA for 1 GPU . export XRT_WORKERS=&quot;localservice:0;grpc://localhost:40934&quot; export XRT_DEVICE_MAP=&quot;CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0|GPU:0;/job:localservice/replica:0/task:0/device:XLA_GPU:0&quot; . If you have 4 GPUs, then use export XRT_DEVICE_MAP=&quot;CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0|GPU:0;/job:localservice/replica:0/task:0/device:XLA_GPU:0|GPU:1;/job:localservice/replica:0/task:0/device:XLA_GPU:1|GPU:2;/job:localservice/replica:0/task:0/device:XLA_GPU:2|GPU:3;/job:localservice/replica:0/task:0/device:XLA_GPU:3&quot; . Why All this? . Having xla TPU support is still a missing and wanted feature of fastai, some months a go Butch Landingin and I joined a hackathon to have a little reusable library it worked as a POC and in some moments we did have something working prior fastai 2 release but later we have found &quot;extrange quirks&quot; that have been difficult to track. And lately we have joined forces with Tanishq Mathew Abraham who has been working in his own support for fastai, so hopefully this time we can make this work. . This was the first capture on Nov 26, see how the name says &quot;compute&quot; while on a 2080 . The good parts . the * It also means that we can have XLA tests running without TPU on a GPU and you don&#39;t need to compile, only get latest build and run on docker GPU, or locally with full compiling as explained above. . XLA GPU optimizations could maybe help your current work? and maybe some things can be tested locally before running full production on the cloud. | The operations sended back to run on CPU locally feel not much slow as they are on TPUs just saying that maybe is more expensive to send ops to CPU on TPU that locally, but havent made a lot of tests and this should be only until all the ops are lowered to TPU. | Have all locally allows to change things like you want, for example I can see the slowness of TPU operations inside the fastai loop with chrome://tracing/ modyfing learner and running the XLA-GPU. And have already found a issue haven&#39;t noticed in latest commits. | . The bad parts . I have been only able to step/debug on python code, not on CPP (but hopefully someone that read this knows a tip to check my vscode settings). | maybe I forgot something more specific in these instructions, but if you find an error, please share. | . References . First hint that xla run on GPU GPU support in PyTorch XLA | This week I spammed the guys at xla Running locally which foes first into running with docker, then locally. | The last missing part, the sed error when building pytorch 1.1.0 from source | Most of the build steps are on xla/CONTRIBUTING | .",
            "url": "https://tyoc213.github.io/blog/xla/fastai/2020/11/28/compiling-xla-locally.html",
            "relUrl": "/xla/fastai/2020/11/28/compiling-xla-locally.html",
            "date": " • Nov 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "librosa 2015 presentation updated calls",
            "content": "From https://www.youtube.com/watch?v=MhOdbtPhbLU in 2015 . I also copy &amp; paste some captures (they look weird) to see how different it was 5 years a go. . %matplotlib inline import matplotlib.pyplot as plt import seaborn seaborn.set(style=&#39;ticks&#39;) from IPython.display import Audio import numpy as np import scipy import mir_eval import librosa import librosa.display . y,sr = librosa.load(librosa.util.example_audio_file()) . . Important: I removed all the outputs of Audio(data=,rate=) because the notebook was more than 19Mb total . Audio(data=y,rate=sr) # cleaned just to lower size of notebook (run again to see embeded player) . waveform . librosa.display.waveplot(y, sr) . &lt;matplotlib.collections.PolyCollection at 0x7fcb930f68b0&gt; . spectrograms . D = librosa.stft(y) D.shape . (1025, 2647) . log_spectrogram = librosa.power_to_db(D**2, ref=np.max) log_spectrogram.shape . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/core/spectrum.py:1544: UserWarning: power_to_db was called on complex input so phase information will be discarded. To suppress this warning, call power_to_db(np.abs(D)**2) instead. warnings.warn( . (1025, 2647) . librosa.display.specshow(log_spectrogram, x_axis=&#39;time&#39;, y_axis=&#39;linear&#39;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7fcb93009370&gt; . librosa.display.specshow(log_spectrogram, x_axis=&#39;time&#39;, y_axis=&#39;log&#39;) plt.colorbar() . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The &#39;basey&#39; parameter of __init__() has been renamed &#39;base&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The &#39;linthreshy&#39; parameter of __init__() has been renamed &#39;linthresh&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The &#39;linscaley&#39; parameter of __init__() has been renamed &#39;linscale&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) . &lt;matplotlib.colorbar.Colorbar at 0x7fcb92f4e0a0&gt; . Constant q transform . direct log-frecuency analysis? . C = librosa.cqt(y, sr) C.shape . (84, 2647) . librosa.display.specshow(librosa.amplitude_to_db(C**2), x_axis=&#39;time&#39;, y_axis=&#39;cqt_hz&#39;) plt.colorbar() . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/core/spectrum.py:1641: UserWarning: amplitude_to_db was called on complex input so phase information will be discarded. To suppress this warning, call amplitude_to_db(np.abs(S)) instead. warnings.warn( . &lt;matplotlib.colorbar.Colorbar at 0x7fcb92e887c0&gt; . TK: add title . librosa.display.specshow(librosa.amplitude_to_db(C**2, top_db=40), x_axis=&#39;time&#39;, y_axis=&#39;cqt_note&#39;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7fcb92d0f0d0&gt; . TK: add title . Spectral features . Spectral features are often used to analyze harmony or timbre. . Usually the product of a spectrogram and a filter bank. . pitch vs class . CQT measures the energy in each pitch. . Chroma measures the energy in each pitch class. . chroma = librosa.feature.chroma_cqt(C=C, sr=sr) chroma.shape . (12, 2647) . librosa.display.specshow(chroma, x_axis=&#39;time&#39;, y_axis=&#39;chroma&#39;) plt.colorbar() . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:822: UserWarning: Trying to display complex-valued input. Showing magnitude instead. warnings.warn( . &lt;matplotlib.colorbar.Colorbar at 0x7fcb906852b0&gt; . TK: add title . Other spectral features includes MEL spectra, MFCC and Tonnetz . M = librosa.feature.melspectrogram(y=y, sr=sr) MFCC = librosa.feature.mfcc(y=y, sr=sr) tonnetz = librosa.feature.tonnetz(y=y, sr=sr) . Audio effects . y_harmonic, y_percussive = librosa.effects.hpss(y) . Audio(data=y, rate=sr) . Audio(data=y_harmonic, rate=sr) . Audio(data=y_percussive, rate=sr) . plt.figure(figsize=(12,6)) C_harmonic = librosa.cqt(y_harmonic, sr) C_perc = librosa.cqt(y_percussive, sr) plt.subplot(3,1,1), librosa.display.specshow(C**(1./3), y_axis=&#39;cqt_hz&#39;), plt.colorbar() plt.subplot(3,1,2), librosa.display.specshow(C_harmonic**(1./3), y_axis=&#39;cqt_hz&#39;), plt.colorbar() plt.subplot(3,1,3), librosa.display.specshow(C_perc**(1./3), y_axis=&#39;cqt_hz&#39;), plt.colorbar() . (&lt;AxesSubplot:ylabel=&#39;Hz&#39;&gt;, &lt;matplotlib.collections.QuadMesh at 0x7fcb92d88d90&gt;, &lt;matplotlib.colorbar.Colorbar at 0x7fcb92df3a00&gt;) . Onsets and beats . onset_envelope = librosa.onset.onset_strength(y, sr) . onsets = librosa.onset.onset_detect(onset_envelope=onset_envelope) . plt.subplot(2,1,1) plt.plot(onset_envelope, label=&#39;Onset strength&#39;) plt.vlines(onsets, 0, onset_envelope.max(), color=&#39;r&#39;, alpha=0.25, label=&#39;onsets&#39;) plt.xticks([]), plt.yticks([]) plt.legend(frameon=True) plt.axis(&#39;tight&#39;) plt.subplot(2,1,2) librosa.display.waveplot(y, sr) . &lt;matplotlib.collections.PolyCollection at 0x7fcb93b6a220&gt; . onset stregth is used to track beats and estimate tempo . tempo, beats = librosa.beat.beat_track(onset_envelope=onset_envelope) tempo, beats . (129.19921875, array([ 4, 23, 43, 63, 83, 102, 122, 142, 162, 181, 202, 222, 242, 261, 281, 301, 321, 341, 361, 382, 401, 421, 441, 461, 480, 500, 520, 540, 560, 579, 600, 620, 639, 658, 678, 698, 718, 737, 757, 777, 798, 817, 837, 857, 877, 896, 916, 936, 957, 976, 996, 1016, 1036, 1055, 1075, 1095, 1116, 1135, 1155, 1175, 1195, 1214, 1234, 1254, 1275, 1294, 1314, 1334, 1354, 1373, 1393, 1413, 1434, 1453, 1473, 1493, 1513, 1532, 1552, 1572, 1593, 1612, 1632, 1652, 1672, 1691, 1712, 1732, 1752, 1771, 1791, 1811, 1831, 1850, 1870, 1890, 1911, 1931, 1951, 1971, 1990, 2010, 2030, 2050, 2070, 2090, 2110, 2130, 2149, 2169, 2189, 2209, 2229, 2249, 2269, 2288, 2308, 2328, 2348, 2368, 2388, 2408, 2428, 2448, 2467, 2487, 2507, 2527, 2547])) . plt.plot(onset_envelope, label=&#39;Onset strength&#39;) plt.vlines(onsets, 0, onset_envelope.max(), color=&#39;r&#39;, alpha=0.25, label=&#39;onsets&#39;) plt.xticks([]), plt.yticks([]) plt.legend(frameon=True) plt.axis(&#39;tight&#39;) . (-132.3, 2778.3, -0.05, 1.05) . beat events are in frame indices . We can convert to time (in seconds), and sonify with mir_eval . beat_times = librosa.frames_to_time(beats) beat_times . array([ 0.09287982, 0.53405896, 0.99845805, 1.46285714, 1.92725624, 2.36843537, 2.83283447, 3.29723356, 3.76163265, 4.20281179, 4.69043084, 5.15482993, 5.61922902, 6.06040816, 6.52480726, 6.98920635, 7.45360544, 7.91800454, 8.38240363, 8.87002268, 9.31120181, 9.77560091, 10.24 , 10.70439909, 11.14557823, 11.60997732, 12.07437642, 12.53877551, 13.0031746 , 13.44435374, 13.93197279, 14.39637188, 14.83755102, 15.27873016, 15.74312925, 16.20752834, 16.67192744, 17.11310658, 17.57750567, 18.04190476, 18.52952381, 18.97070295, 19.43510204, 19.89950113, 20.36390023, 20.80507937, 21.26947846, 21.73387755, 22.2214966 , 22.66267574, 23.12707483, 23.59147392, 24.05587302, 24.49705215, 24.96145125, 25.42585034, 25.91346939, 26.35464853, 26.81904762, 27.28344671, 27.7478458 , 28.18902494, 28.65342404, 29.11782313, 29.60544218, 30.04662132, 30.51102041, 30.9754195 , 31.43981859, 31.88099773, 32.34539683, 32.80979592, 33.29741497, 33.7385941 , 34.2029932 , 34.66739229, 35.13179138, 35.57297052, 36.03736961, 36.50176871, 36.98938776, 37.43056689, 37.89496599, 38.35936508, 38.82376417, 39.26494331, 39.75256236, 40.21696145, 40.68136054, 41.12253968, 41.58693878, 42.05133787, 42.51573696, 42.9569161 , 43.42131519, 43.88571429, 44.37333333, 44.83773243, 45.30213152, 45.76653061, 46.20770975, 46.67210884, 47.13650794, 47.60090703, 48.06530612, 48.52970522, 48.99410431, 49.4585034 , 49.89968254, 50.36408163, 50.82848073, 51.29287982, 51.75727891, 52.221678 , 52.6860771 , 53.12725624, 53.59165533, 54.05605442, 54.52045351, 54.98485261, 55.4492517 , 55.91365079, 56.37804989, 56.84244898, 57.28362812, 57.74802721, 58.2124263 , 58.6768254 , 59.14122449]) . y_click = mir_eval.sonify.clicks(beat_times, sr, length=len(y)) Audio(data=y+y_click, rate=sr) . Temporal structure . c_sync = librosa.util.sync(chroma, beats, aggregate=np.median) c_sync.shape . (12, 130) . librosa.display.specshow(c_sync, y_axis=&#39;chroma&#39;) plt.colorbar() . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:822: UserWarning: Trying to display complex-valued input. Showing magnitude instead. warnings.warn( . &lt;matplotlib.colorbar.Colorbar at 0x7fcb904d6100&gt; . history embedding can add context . chroma_stack = librosa.feature.stack_memory(c_sync, n_steps=3, mode=&#39;edge&#39;) chroma_stack.shape . (36, 130) . librosa.display.specshow(chroma_stack, y_axis=&#39;chroma&#39;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7fcb903be0d0&gt; . recurrence plots show nearest neighbor linkage for each frame. . Chroma recurrence can encode harmonic repetitions . # R = librosa.segment.recurrence_matrix(y, sym=True) . #R = librosa.segment.recurrence_matrix(chroma_stack, sym=True) # diagonal lines indicate repeated progressions # librosa.display.specshow(R, aspect=&#39;equal&#39;) # post processing R can reveal structural components, metrical structure, etc . How to plot the different Rs above? .",
            "url": "https://tyoc213.github.io/blog/librosa/audio/2020/09/26/Librosa-2015-presentation-updated-calls.html",
            "relUrl": "/librosa/audio/2020/09/26/Librosa-2015-presentation-updated-calls.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Reconociendo dígitos en fastai2",
            "content": "Instalar fastai2 . http://dev.fast.ai/#Installing yo prefiero instalarlo como dice ahí en su propio entorno de conda. . git clone https://github.com/fastai/fastai2 cd fastai2 pip install -e &quot;.[dev]&quot; . para que CUDA este disponible python -c &#39;import torch; print(torch.cuda.is_available())&#39; si no lo esta checa que este instalado con nvidia-smi, si esta instalado puede que no se haya instalado la versión correcta de pytorch y los drivers instalados ve como hacerlo en https://pytorch.org/get-started/locally/ . Registrandose en kaggle . Registrarse como usuario en kaggle | hacer pip install kaggle | Obtener tu clave privada de usuario y guardarla en la ruta default | Entrar en https://www.kaggle.com/c/digit-recognizer y dar click en aceptar reglas para poder usar la aplicación de kaggle. | Bajar los archivos con kaggle competitions download -c digit-recognizer y descomprimirlos en su propia carpeta | Importar fastai2 . Importamos el modulo vision de fastai2 . from fastai2.vision.all import * . Vamos a definir algunas variables para poder ponerle nombre a nuestros archivos a enviar y guardar el nombre del modelo que usamos. . VERSION = 4 MODELO=resnet34 NOMBRE_MODELO=&quot;resnet34&quot; BASE_FILE = f&quot;base-{NOMBRE_MODELO}_v{VERSION}&quot; SUBMIT_FILE = f&quot;submit-{NOMBRE_MODELO}_v{VERSION}&quot; FINE_FILE = f&quot;base-{NOMBRE_MODELO}-fine_v{VERSION}&quot; SUBMIT_FINE_FILE = f&quot;submit-{NOMBRE_MODELO}-fine_v{VERSION}&quot; . Fastai2 trabaja muy bien con data loaders, estos necesitan que exista algun archivo en disco, lo cual puede ayudar para datasets grandes, pero tal vez no mucho a su lectura recurrente. Sólo necesitamos preprocesar esto la primer vez, por lo mismo usamos esos ifs para extraerlos si es necesario desde cada row del csv hacia la imagen en disco. . import string # in the same folder I have downloaded with: kaggle competitions download -c digit-recognizer path = untar_data(&quot;file://digit-recognizer.zip&quot;, dest=&quot;.&quot;) #print(path) #print(path.ls()) from PIL import Image from matplotlib import cm import pandas as pd path = Path(&quot;digit-recognizer&quot;) df = pd.read_csv(path/&quot;train.csv&quot;, header=&#39;infer&#39;) df.head() . label pixel0 pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ... pixel774 pixel775 pixel776 pixel777 pixel778 pixel779 pixel780 pixel781 pixel782 pixel783 . 0 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 785 columns . %%time def get_image(csv_row): img = csv_row.reshape(28,28) x = cm.gist_earth(img)*255 return Image.fromarray(np.uint8(x)) def explode_train(): df = pd.read_csv(path/&#39;train.csv&#39;, header=&#39;infer&#39;) imagenes = df.iloc[:,1:].apply(lambda x: x.values, axis=1).values labels = df.iloc[:,:1].apply(lambda x: x.values[0], axis=1) p = Path(path)/&#39;train&#39; p.mkdir(parents=True, exist_ok=True) for idx, l in enumerate(labels): im = get_image(imagenes[idx]) image_name = (str(l)+&quot;_&quot;+&#39;train&#39;+&#39;_&#39;+&#39;&#39;.join(random.choice(string.ascii_lowercase) for i in range(8))) + &quot;.png&quot; im.save(p/image_name) def explode_test(): df = pd.read_csv(path/&#39;test.csv&#39;, header=&#39;infer&#39;) imagenes = df.iloc[:,:].apply(lambda x: x.values, axis=1).values p = Path(path)/&#39;test&#39; p.mkdir(parents=True, exist_ok=True) for idx, l in enumerate(imagenes): im = get_image(imagenes[idx]) im.save(p / f&quot;{idx}.png&quot;) if not (path/&#39;train&#39;).exists(): explode_train() if not (path/&#39;test&#39;).exists(): explode_test() . CPU times: user 106 µs, sys: 13 µs, total: 119 µs Wall time: 81.1 µs . Para leer todas las imagenes generadas, lo haciemos por medio de un DataBlock el cual es un blueprint de donde, como y que vamos a hacer con esas imagenes para obtener finalmente las imagenes en batches o grupos de imagenes (de 64 en 64 por default) y la carga de las imagenes como tal se hace por medio de data_loader = data_block.dataloaders(path) el cual en este caso también aprende el &quot;vocabulario&quot; a aprender ya que se ha especificado que el segundo bloque es un bloque de categorias a diferencia del primero que es de Imagenes. . get_my_labels obtiene a partir de el archivo de imagen leído el nombre y regresa el dígito al que pertenece del vocabulario. . %%time def get_my_labels(fname): return int(fname.name[0]) dblock = DataBlock( splitter = RandomSplitter(), item_tfms = Resize(224), blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, get_y = get_my_labels ) dls = dblock.dataloaders(&quot;digit-recognizer/train&quot;) dls.vocab . CPU times: user 4.12 s, sys: 549 ms, total: 4.67 s Wall time: 4.81 s . (#10) [0,1,2,3,4,5,6,7,8,9] . show batch . Podemos ver que realmente este encontrando las imagenes mostrando un batch de imagenes. . dls.show_batch() . Cargar un modelo y entrenarlo . En fastai2 podemos cargar un modelo ya entrenado y reusarlo para ajustarlo a la tarea que queremos llevar a cabo. . learn = cnn_learner(dls, MODELO, metrics=error_rate) . Nuestro modelo en este momento ya tiene una estructura la cual se puede sumarizar así . learn.summary() . Sequential (Input shape: [&#39;64 x 3 x 224 x 224&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 64 x 112 x 112 9,408 False ________________________________________________________________ BatchNorm2d 64 x 64 x 112 x 112 128 True ________________________________________________________________ ReLU 64 x 64 x 112 x 112 0 False ________________________________________________________________ MaxPool2d 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 73,728 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 8,192 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 294,912 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 32,768 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 1,179,648 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 131,072 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 512 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 512 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 1024 0 False ________________________________________________________________ BatchNorm1d 64 x 1024 2,048 True ________________________________________________________________ Dropout 64 x 1024 0 False ________________________________________________________________ Linear 64 x 512 524,288 True ________________________________________________________________ ReLU 64 x 512 0 False ________________________________________________________________ BatchNorm1d 64 x 512 1,024 True ________________________________________________________________ Dropout 64 x 512 0 False ________________________________________________________________ Linear 64 x 10 5,120 True ________________________________________________________________ Total params: 21,817,152 Total trainable params: 549,504 Total non-trainable params: 21,267,648 Optimizer used: &lt;function Adam at 0x7fb52353ab00&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group number 2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Podemos ver las predicciones actuales antes de reajustar los parametros de la red. En rojo se muestran los errores en verde los que estan bien. Los aciertos más bien son aleatorios. . Los resultados como se espera no pueden reconocer los patrones ya que esta inicializada para otro tipo de tarea. . learn.show_results() . Ahora podemos entrenarlo con fine_tune . El método fit sirve para ejecutar una serie de pasadas sobre los datos de entrenamiento. . %%time learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.227946 | 0.135213 | 0.040000 | 01:16 | . epoch train_loss valid_loss error_rate time . 0 | 0.035625 | 0.025032 | 0.007024 | 01:43 | . CPU times: user 2min 15s, sys: 26.2 s, total: 2min 42s Wall time: 3min . learn.show_results() . Nuestro modelo en este momento ya puede asertar con más confianza el dígito que se le esta pasando . learn.predict(Path(&quot;digit-recognizer&quot;)/&#39;test&#39;/&quot;1111.png&quot;) . (&#39;2&#39;, tensor(2), tensor([8.4126e-10, 8.9289e-08, 1.0000e+00, 7.2959e-07, 2.4069e-09, 2.0494e-08, 2.9449e-09, 1.0405e-07, 2.0946e-07, 1.2205e-10])) . Si volvemos a checar los resultados podemos ver que ciertamente ya puede reconocer los patrones. . Veamos cuales son las imagenes que más costaron reconocer. . interp = Interpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(15,10)) . Para guardar los resultados obtenidos y mandarlos a kaggle, basta con hacer lo siguiente . def predict_test(the_learner, file_name): p = path/&#39;test&#39; print(f&quot;predicting {len(p.ls())} in {p}&quot;) l = [] for idx, img in enumerate(p.ls()): fname = p/f&quot;{idx}.png&quot; pred = the_learner.predict(fname) # la predicción contiene todo el resultado, la predicción esta en el elemento 0 l.append( [idx+1, int(pred[0])] ) if idx % 2800 == 0: print(f&quot;{[idx, pred[0]]}...&quot;) df = pd.DataFrame(l) h = [&quot;ImageId&quot;,&quot;label&quot;] df.to_csv(f&quot;{file_name}.csv&quot;, header=h, index=False) if True else print(&quot;** skipped save **&quot;) print(&quot;done!&quot;) . Removemos el callback del loader . %%time learn.remove_cb(learn.cbs[2]) predict_test(learn, BASE_FILE) . predicting 28000 in digit-recognizer/test [0, &#39;2&#39;]... [2800, &#39;4&#39;]... [5600, &#39;8&#39;]... [8400, &#39;5&#39;]... [11200, &#39;4&#39;]... [14000, &#39;3&#39;]... [16800, &#39;1&#39;]... [19600, &#39;5&#39;]... [22400, &#39;9&#39;]... [25200, &#39;3&#39;]... done! CPU times: user 53min 39s, sys: 1h 36min 21s, total: 2h 30min Wall time: 2h 49min 33s . El Learner que actualmente se encuentra en learn es muy bueno para quedar cerca de los primeros mil competidores, lo que tenemos que hacer ahora es mejorarlo poco a poco. . Pero antes de esto vamos a salvarlo de 2 formas: . exportandolo el cual requiere que despues se vuelva a cargar y crear otro cnn | salvar y cargar de manera directa | . Guardamos el modelo actual para poder cargarlo despues via load. . learn.save(BASE_FILE) if True else print(&quot;No se a guardado el archivo&quot;) . Fine tunning . Ahora se puede cargar desde donde se quedo el paso guardado por save anterior, pero debe de contener datos a los cuales referirse, por eso se carga a travez de un modelo instanciado igual que antes, sólo que este ya esta &quot;entrenado&quot; a la tarea actual. . l2 = cnn_learner(dls, MODELO, metrics=error_rate) . Una vez instanciado igual que el modelo que guardamos, se puede cargar el mismo desde el archivo que se guardo con learn.save . l2.load(BASE_FILE) . &lt;fastai2.learner.Learner at 0x7fb5202f10d0&gt; . Buscamos un buen learning rate . l2.lr_find() . SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=6.309573450380412e-07) . Descongelamos el modelo entrenado para que se pueda entrenar nuevamente y usamos el learning rate que encontramos anteriormente . %%time l2.unfreeze() l2.fine_tune(8, 1e-2) . epoch train_loss valid_loss error_rate time . 0 | 0.080212 | 0.057889 | 0.010238 | 01:10 | . epoch train_loss valid_loss error_rate time . 0 | 0.074369 | 0.067521 | 0.013690 | 01:36 | . 1 | 0.082539 | 0.081573 | 0.018690 | 01:36 | . 2 | 0.058147 | 0.052700 | 0.015238 | 01:35 | . 3 | 0.042055 | 0.044290 | 0.009048 | 01:34 | . 4 | 0.029418 | 0.040308 | 0.009048 | 01:33 | . 5 | 0.008509 | 0.022320 | 0.004286 | 01:34 | . 6 | 0.003744 | 0.021935 | 0.004286 | 01:36 | . 7 | 0.000918 | 0.023328 | 0.004524 | 01:35 | . CPU times: user 10min 46s, sys: 2min 52s, total: 13min 38s Wall time: 13min 53s . Guardamos el nuevo modelo ajustado . l2.save(FINE_FILE) . Y lo usamos para predecir eliminando también el callback del progress bar . l2.cbs, l2.cbs[2] . ((#3) [TrainEvalCallback,Recorder,ProgressCallback], ProgressCallback) . %%time l2.remove_cb(l2.cbs[2]) predict_test(l2, FINE_FILE) . predicting 28000 in digit-recognizer/test [0, &#39;2&#39;]... [2800, &#39;4&#39;]... [5600, &#39;8&#39;]... [8400, &#39;5&#39;]... [11200, &#39;4&#39;]... [14000, &#39;3&#39;]... [16800, &#39;1&#39;]... [19600, &#39;5&#39;]... [22400, &#39;9&#39;]... [25200, &#39;3&#39;]... done! CPU times: user 39min 42s, sys: 1h 35min 30s, total: 2h 15min 13s Wall time: 2h 36min 45s . l2.validate() . (#2) [0.023327725008130074,0.0045238095335662365] . Exportar a producci&#243;n . Si no queremos seguir reentrenando nuestro trabajo, podemos sólo exportar el modelo para que se use en predicciones al cargarlo. . learn.export(BASE_FILE) if False else print(&quot;no se ha exportado&quot;) . no se ha exportado . l2.export(FINE_FILE) if False else print(&quot;no se ha exportado nada&quot;) . no se ha exportado nada .",
            "url": "https://tyoc213.github.io/blog/fastai2/2020/05/26/Reconocer-d%C3%ADgitos.html",
            "relUrl": "/fastai2/2020/05/26/Reconocer-d%C3%ADgitos.html",
            "date": " • May 26, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ":) .",
          "url": "https://tyoc213.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tyoc213.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}