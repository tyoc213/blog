{
  
    
        "post0": {
            "title": "Dos and don'ts with invitation to make study groups",
            "content": "How it started . I started learning deep learning 2 or 3 years ago watching the Jeremy videos. I learned some things but I didn’t code, watching videos isn&#39;t enough, sure I learned to use Jupyter lab and were able to follow some tutorials and understand some concepts but still lots of things were still vague. . My first study group ever was for reading the fastabook, were we joined the meeting read silently the chapter in turn and then try to answer 1 or 2 questions that were on the lines of why or how and don&#39;t focus on what questions. It took some months to read 18 chapters (one chapter per week). As all study groups, it started with more people than the ones who could finish it. It was my first time finishing a technical book in quite some time and feel that I have learned and appreciated things I could not do only watching videos. . By the end last months of the 2020 I have already think of taking a look at walk with fastai 2020 course but I was &quot;slow&quot; and not finding time to do it, was only a &quot;target&quot; or a TODO item. So after I joined a meeting on the voice channel of fastai discord, I had time to talk with Zachary and ask him if it was &quot;fine&quot; to start a new study group for his course and he liked the idea. So the first thing I did was to watch the videos fast forward and make the timings for youtube at same time Zach was working on updating and moving all to this new site with a submenu and all the work needed to keep up to date working notebooks. . When I posted the invitation for this study group, I chose to make the study group meet 2 times a week: one to watch the video and the next one to execute the notebook(s) for the class. I think this is hard because people need to keep 2 slots of their week time, but what I was trying to make is like make the watch the video &quot;optional&quot; and run it yourself not optional (so you can still watch the video on your own). . Meetings . So the time catched up in January and Zachary had already moved all the notebooks to the new site. I will write a little about the first meeting(s) and follow ups and place some Do&#39;s and Don&#39;ts. . tools . Instead of using the video sharing on discord, I decided to use metastream to share the playback of the video (I also had at the side a copy of the &quot;blank&quot; times of the video to skip easely) | For the firsts sessions I used locally jupyter lab and VSCode (which I also showed how I worked with nbdev and VSCode hand in hand), but after some updates on local drivers and pytorch don’t providing support for the newest drivers I ended using colab (you can only have 2 running notebooks on colab). | Discord was used to talk/discuss the video and most of all in the second session to run your own notebook. | . First weeks . For the first meetings more than 10 people joined, they installed the required plugin by metastream and watched the shared play it was a nice start and for the &quot;run the notebook meeting&quot; I decided not to take a look more than the video itself and did not run the notebook before hand so the first meeting I was only able to cover the first pet session instead of the 3 notebooks and I used 1:30 hours (some people left because as I have said sometimes time constraints are difficult). . In that presentation I showed how to use Jupyter, how to see where the code is from visual studio code and if I&#39;m not wrong a little about how to work on nbdev and vscode one side or the other. I also skip a little and showed how to call show training loop and maybe other things that were explained later in the videos. . Also it was the first time I was getting feedback from participants, while I have been a user of fastai I was not able to completely understand the progress bar and some one hinted to me that it shows the current accuracy, how many samples of the batch are and the remaining time over runtime. Also got feedback to understand the recorder plot learning rate, all these things I have seen in videos and ran in notebooks before, but didn’t understand the little details. . middle sessions . Watching the video together was mostly just that but on Thursday for me it still took quite some time to finish just one notebook, still the discussions were nice and extended, always learning something from the people who shared their knowledge or taking more internally something I have seen before. . I started to see more of the patterns of the library as I explained them and got more used to seeing the common parts of the high level API, and understood a little better the difference between having and instance of a dataloader taht is used to pass data to the learner in batches and the “template” needed to make the instance which is a DataBlock. . Also at some point people started to show less and finally the people that ended where the ones showing up meet by meet. . I have also received the feedback that I should run notebooks beforehand to see if there are possible errors instead of tackling them at the moment and with this I could start covering more than 1 notebook in 1 hour because sometimes running a specific line and &quot;fixing&quot; the issue at the moment which sometimes result in orienting the talk to other issues instead of the notebook itself. . final sessions . As with all study groups (and maybe a lot of things in life) at end there were a lot less people the people that committed till the end, probably just because making time to have a schedule on your own time is not easy to keep, but also could be my bad handling of the group, or that 2 meetings a week is a lot, or maybe I didn&#39;t provide enough good information and was boring. But as other people have said, don&#39;t worry about that, is what happens with these study groups and lots of things that surge out of pure interest. . From time to time people more knowledgeable showed up not only the ones attending since the start and shared some of their knowledge, that is always appreciated in any meeting. . And finally the last sessions took place and we finally did it from start to finish, and for me it was a great experience being the first time doing something like this, learning from people mostly, it was also a way to go over the course, because as I have said I only have think to go over the course for some time until I did it this way. . Nice things that happened was that Zachary also joined some times and molly (which leaded the first study group I joined) showed up and explained more advanced things to fellows, things that I we haven’t thought about or knew how to explain, so it was nice. . Do&#39;s . Record the meetings there will be always useful information for newcomers or even to middle or advanced users (sadly I didn&#39;t do this thinking that people will be mad if I recorded the meetings, but while running the study group, people started to ask about recordings). | Do run notebooks beforehand so that errors pop out if any exist and solve it beforehand (you can explain them in session and some of them could be as easy as downloading an image). Also being ready to present and is not &quot;cheating&quot;. Is being ready to explain ;). | Write a blogpost for each meeting or keep track of each meeting in a note, basically keep track of the things you did learn or understand better after the meeting (is like sending notes after a meeting just to check that those points are important). I didn&#39;t keep track but learned a lot of things. Sadly this is the only blog post I have made about these meetings, but I&#39;m sure I will forget some things if I don&#39;t put them in practice soon or write about them. This tip can also help people watching the streams or course. | Be honest about what you don&#39;t know or understand, there are people that have the knowledge or people that understand better or in a different way so invite them to share the knowledge. | Be clear about dates/times, sadly the time we did our study group it didn&#39;t fit people in Europe (3am for them), but here is where recordings could help. | Focus on a level, or make it clear, for me this was more beginner friendly, but interactions with people make it advanced or more helpful than just beginner friendly (and it is always good that someone with more experience joins). On that note, if you have something that you can&#39;t answer and there is no one around that can explain it, feel free to try to ping friends on chat or someone on the chat at the moment so that they can join at the moment (or answer later in text). | . | Submit bugs or changes to the repo you are using for reference so that it keeps up to date, the maintainer will like help with maintaining source in good shape to be helpful as it was intended when created. | Even if you don&#39;t understand something, having or attending a study group will expose you to the opportunity to exchange other people&#39;s point of view and understanding of the subject. As someone pointed in one meeting &quot;I have been working with pandas the whole time, but found 1 call on the notebooks that I didn&#39;t know, so it is always good to watch other people how they use a tool you always discover something you aren&#39;t using&quot;. | If possible have a target to apply what all are learning in the study group, one of the things I could not solve was the invitation or wondering of people asking if we will do a kaggle competition, analyse something, in other way people were asking “how we can apply this we are learning now?”, sure in the future what has been learned will contribute to solve something, but for this time I didn&#39;t have a project, kaggle comp or something to give to the attendees. | Try to make questions that ask for: why and how, questions that can move you or others to learn furter. | . Dont&#39;s . Not keep track of what you learned, not just read, but make a note on what you understand now that didn&#39;t before, make your own resume. | As organizer don&#39;t forget about the meeting and if you will not have time because something show up do one of this: notify on time (at least 30 min before and if possible at start of the day or even before that). | ask if someone wants to lead a meeting (hopefully within a week from the event), you can invite your own attendees probably making it their first time leading an online meeting. I didn’t do this, but it would be nice to invite people to take the lead on something if they want to participate like that. | . | Say things that you will not meet on time. | Improve if possible what you are following, but I have sended one PR to Zach, there were a lot of little PRs that I wanted to do as improvements or extra hints but haven&#39;t done. | . Maybe dont&#8217;s . Maybe doing 2 meetings a week is too much, I&#39;m not sure, but maybe ask or make clear that one of the meetings they can do on their own and the other is more participative. Also I have seen some study groups have 2 meetings to tackle bigger books in “little time”. . Finally an Invitation to make more study groups . So I guided a study group without knowing much and without experience as people were able to see it, you don’t need to have perfect knowledge beforehand (sure it is nice if you are and can provide high level extra hints it is always neat when someone with nice skills share their knowledge openly) but also learning with ppl reading first time is nice. . So my final point is to make an invitation to use more of the audio channels with video streams on discord fastai, make more study groups if not on fastai in other related subjects. I think it is a nice opportunity to share lots of things interactively. . For example I have joined a blender 3d discord and there you will see they have like 10 channels where people jump on and off to share what they are working and sometimes people help, yeah it would be hard to get to that level of sharing and maybe to chaotic... but at least study groups could be created it also helps you to present to a public a talk and organize yourself a little a week.. I show one of this Discord capture of some blender users sharing at the moment what they are doing and asking for feedback/help: . . Indeed forums and discord chat are nice (and you have history), but also watching or talking to people is important because talk is one aspect of human life, also you can get carried away more easily talking than writing (because you can edit, delete and so on). . Even if you are in another area and not DL, always having people to talk to about a subject matter is important, because you don&#39;t only get your point of view. For me this was my first time leading a study group and I apart from learning I also noticed things I have not put attention to, sure I&#39;m still a novice, but that is not a stopper to organize online meetings at the end you are also learning and probably most of us are not teachers that need to know beforehand what is being teached, that is why it is a study group keep people interacting and maintain the schedule so that there is a space to talk about the subject at hand. . :) .",
            "url": "https://tyoc213.github.io/blog/fastai/2021/04/10/dos-and-donts-with-invitation-to-make-study-groups.html",
            "relUrl": "/fastai/2021/04/10/dos-and-donts-with-invitation-to-make-study-groups.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Lowering SiLU in pytorch/XLA",
            "content": "Introduction . As a follow up of compiling pytorch locally, the next objective was to lower an operation, but the documentation on pytorch and XLA is almost the same (think of XLA as an extension of Pytorch), you can check the OP_LOWERING_GUIDE which basically is OP LOWERING GUIDE from pytorch, probably you will get it at first hand, but I was not exactly sure what to do next for example I didn’t know when to modify gen.py (which you don’t need if it is already in see and other things. . What says the op lowering guide? . Now that I have lowered an op, I think what the lowering guide says is something like this: . That you need to have an environment to compile/test/run | That you need to understand the operation SiLU | That you need to implement new operations copying verbatim the signature from header and cpp files to aten_xla_type.h/cpp from aten_xla_type_default.h/cpp | You will use XLATensor tensors as input and output as pytorch tensor (ATen means “a tensor library”) | From this level in tensor_methods will be ir ops that are constructed from pytorch to use the tensorflow XLA compiler ops (which I think it is not exhaustive? not all is output to the doc?). This operations include for example the * I used and xla::Sigmoid | They also list 2 Add inverse op and test and Implement the lowering for HardSigmoid and HardSigmoidBackward which could help and are a good start too. . The opportunity . I asked in which op I can contribute and JackCaoG suggested the ones that were in the backlog, unfortunately they were probably not entry level, by fortune little time after the opportunity showed up here and this SiLU op was good for entry level because as description says it uses as base Sigmoid which is already lowered also JackCaoG said it should be mostly like sigmoid or log_sigmoid and from the description on the documentation it looked like that. . I really don’t know much about pytorch and sure I didn’t know of SiLU before, but the signature of SiLU was not like those provided as base, I finally checked the other ops that ended with _out( as example arange_out . Implementation . As always, create a new branch and don’t forget to update to the latest master in pytorch and XLA (which in my case caused some behaviour about synching the repos). . 1. Create the base . First commit https://github.com/pytorch/xla/pull/2721/commits/c16fedbbee3662d3470629dc7fff51c63dd60855 . It provides the base and starting point: . Copy the signature from the header and implementation of to `` | Copy the body from the header to . | It also reused at a higher level the Sigmoid as expected, the problem with this is that the generated graph for the compiler will list this as a x * sigmoid(x) (which was basically this input.GetIrValue() * ir::ops::Sigmoid(input.GetIrValue());) instead of a SiLU in the node graph. | But this implementation was good enough to compile without errors and actually run my rudimentary base test that output the same values for cpu implementation and XLA implementation . import torch from torch.nn import SiLU import torch_xla.core.xla_model as xm dede=xm.xla_device() m = SiLU() m = m.to(dede) input = torch.randn(2) input2 = input.clone() # this is on CPU input = input.to(dede) output = m(input) print(output) print(output.device) # should print xla m2 = SiLU() print(&quot;normal&quot;) print(&#39;input2&#39;, input2) o2 = m2(input2) print(o2) # this should match print above . The review of the PR suggested the next step, which is: . 2. Go deeper with the lowering . Now that you have a base go deeper and make your node appear. . Because when people is debugging the generated graph of tensor ops in XLA with the previous implementation it would be better if calling SiLU would generate a SiLU node and not x * sigmoid(x) as the previous step. . Second commit https://github.com/pytorch/xla/pull/2721/commits/c16fedbbee3662d3470629dc7fff51c63dd60855 . It shows how to add an op that will be used as a node in the generated graph for the operation. . It adds the operation to ops.h/cpp | It converts from tensors to XLA tensors and back. | It reuses at this level the implementation of SiLU (which is valid because you have already named the node at this level) which is node.ReturnOp(xla_input * BuildSigmoid(xla_input), loctx) and provides a “name” for the node with GenericOp(OpKind(at::aten::silu), {input}, input.shape(), std::move(lower_fn)). | My first approach was to repeat all so I duplicated the sigmoid implemented in elementwise.h/cpp and used that but the review of the PR suggested that I can call sigmoid because the node was already a SiLU so it doesn’t matter if I reused what was already there at that moment. I corrected with an amended and just reused Sigmoid instead of my SiLU in elementwise, making the commit writes 2 less files than amended commit. . The backward pass . This operation didn’t include a backward pass, you should implement it if the header contains the forward and the backward pass, this was more a _out operation that is also used for in place methods. . Note: I haven’t seen how this dispatch works, but I guess works like simple inheritance, when aten_xla_type don’t provide the method then the ones from aten_xla_type_default are used (which is the CPU implementations and fallbacks). But see that the type AtenXlaType is not a subclass aten_xla_type” . “Also note that aten_xla_type_default which is auto generated after build in some stage because it is not in repo and is ignored in .gitignore. So it should be other type of dynamic dispatching somewhere deep in the code. . Conclusion . Lowering an op is difficult, but practice does help and easy tasks does too. You also need to provide a test case, which probably is just to take a template from a previous one (because test from CPU pytorch are used as base). . There are different things you need to know at less a little: pytorch, XLA, C++ (to see how the default operation is implemented), even some cuda if you can read that and take it as reference apart from the default CPU implementations and “cpu_fallback” (which I still don’t know how they differ from CPU implementations or when they are used). . Hopefully this little explanation will help another person who wants to contribute lowering ops and understand a little better what is explained on the op lowering guide. .",
            "url": "https://tyoc213.github.io/blog/xla/2021/01/24/op_lowering_pythorchXLA.html",
            "relUrl": "/xla/2021/01/24/op_lowering_pythorchXLA.html",
            "date": " • Jan 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Audio y auto estudio interview",
            "content": "Here&#39;s my interview with Robbert Bracco @MadeUpMasters (author of &quot;Things Jeremy Says to do&quot; on the @fastdotai forums) all about deep learning and https://t.co/wANZD5nFcn (library) applied to audio, self-study in ML.Audio: https://t.co/T9CeIBtUKrVideo: https://t.co/ZCJzuxCIwG . &mdash; Sanyam Bhutani (@bhutanisanyam1) September 1, 2019 Parte 1 . https://youtu.be/k-gZAyg5ib8?t=0 Introducción . https://twitter.com/madeupmasters | https://forums.fast.ai/t/things-jeremy-says-to-do/36682 | . “Para lograr ser autodidacta se requiere ser consistente y un desafío balanceado”. . Y hay que ver en teoría en una escuela “quieren que construyas un compilador antes antes de hacer una pieza útil de software” . “La AI puede ser más flexible para detectar cambios en un audio que la programación tradicional” lo cual podría ser complicado de programar de manera imperativa o funcional. . tomó los cursos . https://cs50.harvard.edu/college/2021/spring/ ó https://online-learning.harvard.edu/course/cs50-introduction-computer-science para principiantes en computación y amplio | https://www.coursera.org/learn/algorithms-part1 | https://www.coursera.org/learn/algorithms-part2 y probablemente sirva de algo el código de este libro https://algs4.cs.princeton.edu/home/ | https://www.coursera.org/learn/machine-learning by Andrew Ng | . “focus on building not on theory” “top-down approach”, en otras palabras es más practico hacer cosas que entender el trasfondo que puede ser más complejo y requerir de más análisis. . parte 2 . https://youtu.be/k-gZAyg5ib8?t=986 brincar a una competencia sin saber nada te da la libertad de experimentar lo que se te ocurra. Un punto interesante es que si tienes suficientes bases puedes explorar más libremente que sabiendo las herramientas default usadas o los modelos usados por defecto “un periodo de creatividad libre”, una de las estrategias que tomó en ese tiempo fue pasar la onda de audio directo al modelo y posteriormente se dió cuenta que tendría que conocer más acerca del procesamiento de señales para poder sacar más información del audio. . “It is easy to get addicted to online classes” se puede entrar en un “ciclo infinito” de aprender que puede bloquear el de aplicar o hacer algo. . https://hackernoon.com/how-not-to-do-fast-ai-or-any-ml-mooc-3d34a7e0ab8c . “What should I don next? is to see what you have in your head and see what is stopping you, then learn that”, enforcarse en aprender sólo las cosas que te faltan por aprender es mejor, es como el eslabon más débil de la cadena siempre es el que se puede romper, hay que aprender a hacer objetivos basados en los “bloqueos” que nos encontramos en el camino. . Kaggle te puede aislar hacia resolver el problema a la mano en vez de toparte con lo que es construir tu propio proyecto porque ya tienes de alguna forma los datos procesados y un objetivo, pero si no lo tienes todo de inicio puede resultar en otra forma de aprendizaje (¿talvez no es posible?, ¿los datos afectan?). Es mejor si se pueden hacer las dos cosas Kaggle+proyecto(s) propio(s). . http://christinemcleavey.com/musenet/ . parte 3 . https://youtu.be/k-gZAyg5ib8?t=1356 . El objetivo de fastai_audio es contruir un modulo que sea compatible con fastai con la misma usabilidad. . Resnets y densenets parecen funcionar bien con FFTs y audio. Y se puede usar transfer learning desde imagenette aunque no tenga nada que ver con audio, probablemente porque las primeras capas reconocen líneas, direcciones y otras características básicas. . Y a base de prueba y error los defaults en fastai_audio han sido puestos en la librería. . Y en cuando a hacer cosas, es mejor hacerlas aunque tengan errores o no sean perfectas (o incluso aunque no las entiendas), talvez alguien más pueda ver el error y corregirlo. Si se espera a ser experto en el area antes de empezar algo, pues por lo menos falta llegar a ese punto primero sin hacer nada práctico de antemano. Y es bueno tener a alguién o algun recurso en quien confiar ya sea una persona, un foro en general alguién a quien poder preguntar aunque tal vez no tengan todas las respuestas. . (SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition][https://arxiv.org/abs/1904.08779] y https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html . parte 4 . https://youtu.be/k-gZAyg5ib8?t=2730 . “consistency matters above all” la consistencia importa más que cualquier otra cosa, poner esfuerzo constante con días de descanso es mejor que hacer mucho en este momento, dejarlo, volver y que sobre esforzarse al punto de “quemarte”. “Intellectual work is exhausting” el trabajo intelectual es agotador, “I’m working from 8:00 a.m. to 2:00 p.m.” trabajo de las 8:00 a.m. a las 2:00 p.m. Con un tiempo más límitado tienes que enfocarte en hacer ciertas cosas y tomar decisiones en ese tiempo sin tener todo el tiempo del día para cosas “fribolas”. .",
            "url": "https://tyoc213.github.io/blog/fastchai/fastai/2021/01/09/fastchai-rbracco-audio-y-estudio-propio.html",
            "relUrl": "/fastchai/fastai/2021/01/09/fastchai-rbracco-audio-y-estudio-propio.html",
            "date": " • Jan 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "¿Cómo reconocer las buenas ideas de investigación que valen tu tiempo y esfuerzo?",
            "content": "How do you recognize good research ideas that are worth your time and effort? And how do you evaluate whether the hypotheses of others make sense? Here is my take:1/ pic.twitter.com/LKy8INNCRm . &mdash; Cecile Janssens (@cecilejanssens) February 24, 2020 Recordando que el método científico . . Hay una parte que es inductiva (se proponen hipotesis) y otra que es deductiva (se comprueba rigurosamente la conclusión a partir de las hipótesis). . Doing science means following the scientific method. The hypotheses we choose to investigate (also in hypothesis-free research) follows from the rigor of our background research and a process that is called inductive reasoning. pic.twitter.com/ojK18UTVK2 . &mdash; Cecile Janssens (@cecilejanssens) February 24, 2020 La parte deductiva incluye: hacer una pregunta, investigar, hacer una hipotesis. La parte inductiva incluye: construir un experimiento para probar la hipotesis, analisar los datos/resultados y sacar una conclusión . This case can be strong or weak. The case is cogent if we can rely on the premises to be true. A case can be made stronger with more supporting evidence from more background research, and weaker e.g., if new evidence contradicts. pic.twitter.com/dz4ncNTFX5 . &mdash; Cecile Janssens (@cecilejanssens) February 24, 2020 Entonces, las premisas por lo menos antes de empezar a trabajar deberían de parecer validas (si no se tienen las pruebas) o en otras palabras ser “solidas” (“sound” en Inglés). . Here are examples of evidence that are essential to justify that the diet might work (also the study protocol paper that they cite did not provide this evidence). With a weak case, an RCT is more likely to show no benefit of the intervention (the diet didn&#39;t work). pic.twitter.com/q8iiBKjofx . &mdash; Cecile Janssens (@cecilejanssens) February 24, 2020 Tambien si basas tu trabajo en otros que no proveen pruebas, talvez sea indicación de poco trabajo previo o mal trabajo previo. . Entonces, se debe buscar tener un fuerte argumento a favor o por lo menos una corazonada fuerte de que al final del camino existe la razon para ocupar tu tiempo en ese argumento respecto a la pregunta inicial. .",
            "url": "https://tyoc213.github.io/blog/note/2021/01/02/reconociendo-una-buena-idea-de-investigacion.html",
            "relUrl": "/note/2021/01/02/reconociendo-una-buena-idea-de-investigacion.html",
            "date": " • Jan 2, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Finding nemo a bug journey",
            "content": "What happened at first . The major part of the last months we were trying to solve one bug that we didn&#39;t know how to solve until this days, some time before fastai version 2 release having a POC and passing the hackathon we participated we left a little the project, but when we came back to it, there were some extrange things happening, some models where not training and apparently others where training, we didn&#39;t understand what happened, in that time we thought we introduced an error in our code some way. . The journey . So, these last few days since I had locally installed XLA and been able to run things, I planned to take a new round to find our bug and it was a perfect opportunity to test what having locally installed pytorch+xla+fatai_xla_etensions could do. . So I passed a lot of assumptions to finally find the solution. . So after having locally installed XLA, one of the first things I wanted to test is if locally I could reproduce the error, and I could! | The first one was that the error was in our code, but having locally XLA allowed me to test the exact same code changing the device to either: CPU, CUDA or XLA. So I ended up having 2 data loaders and 2 trains on the same python file. Also one main point was that I wrapped a Adam but from pytorch with OptimWrapper and it trained correctly so I was more suspicious of differences between the optimizer from fastai and the native to pytorch because there is one know difference about __getstate__ that is also a requirement for TPU Pods. | In the past we have also thought that it was a freeze unfreeze problem, but it was also discarded, so this time I was checking the optimizer, but could not find why the params were not training even when looking under the lens. | But after more testings and so on, I see that the second example started to train correctly while the first on the file not, and it was that with all the fresh runs, so I thought it was a problem with the learner, but could not find a &quot;real problem&quot; so I returned back to the optimizer and all, but this time I have a new &quot;tool&quot; I learned, counting the trainable parameters so, the trainable parameters their gradients are updated when you call backward, so I started to count there and for the first example they where always zero while for the second run since start, they have a number. So the next task was to find why on the first the trainable parameters are always zero and the second not. | But I still didn&#39;t get why one model was training and the other one not! . I passed _BaseOptimizer, Optimizer, Learner and others and still could not find the problem, so I decided to compare models and found the problem! I updated the example I found in pytorch forums https://discuss.pytorch.org/t/two-models-with-same-weights-different-results/8918/7 the original one at first run did break because it compared tensors not on same device, so it threw error, I modified it so that it prints them nicely instead of being caught in that error. . def compare_models(model_1, model_2): models_differ = 0 for key_item_1, key_item_2 in zip(model_1.state_dict().items(), model_2.state_dict().items()): if key_item_1[1].device == key_item_2[1].device and torch.equal(key_item_1[1], key_item_2[1]): pass else: models_differ += 1 if (key_item_1[0] == key_item_2[0]): _device = f&#39;device {key_item_1[1].device}, {key_item_2[1].device}&#39; if key_item_1[1].device != key_item_2[1].device else &#39;&#39; print(f&#39;Mismatch {_device} found at&#39;, key_item_1[0]) else: raise Exception if models_differ == 0: print(&#39;Models match perfectly! :)&#39;) . And that was the solution to the problem, I focused on seeing why the models parameters were on different devices. At the end I have something like (remember I don&#39;t need to patch the optimizer because I have all installed locally). . def create_opt(self): print(&#39;trainable count before&#39;, len(self.all_params(with_grad=True))) self.opt = self.opt_func(self.splitter(self.model), lr=self.lr) print(&#39;trainable count after&#39;, len(self.all_params(with_grad=True))) if not self.wd_bn_bias: for p in self._bn_bias_state(True ): p[&#39;do_wd&#39;] = False if self.train_bn: for p in self._bn_bias_state(False): p[&#39;force_train&#39;] = True . and . print(&#39;trainable count before backward&#39;, len(self.all_params(with_grad=True))) self(&#39;before_backward&#39;) print(&#39;trainable count before backward&#39;, len(self.all_params(with_grad=True))) self._backward() self(&#39;after_backward&#39;) . So at the end I see that even that the model is moved later to the device, the first time when splitter=trainable_params in self.opt = self.opt_func(self.splitter(self.model), lr=self.lr) inside create_opt it is not there, so the parameters where stuck on CPU while the the data and the model is later moved to the XLA device. . This does not affects GPUs, but thinking about it could mean also something about the pickable behaviour of xla tensors, specially the optimizer, but that is an history for another time, right now, we have again a simple lib that works for Single-device TPUs where you need to modify zero code from fastai. . Conclusion . So the model need to be in TPU/XLA device before their parameters are taked by splitter on Optimizer initialization, I guess we assumed some things in between then and now. At the end it was not exactly an error but was. But sure it was difficult to track, now knowing what it is is solved and we can continue forward. . I hope to add in the next release a show_lowering_ops (or similar) to print the counters if you have hit some of those and it is easy to print in a model that runs with this activated. The MNIST demo should be working again don&#39;t forget to peek at fastai_xla_extensions. . EXTRA NOTE: But there was error because XLA model on CPU not trained when updating backward from data operations on XLA device? well, now I think that XLA worked on TPU with model and data copied to TPU on first time but somehow our model got stuck on CPU so not trained, it became another model separate from the execution happening on TPU (I can think of pickable things, but that is unknown at the moment) .",
            "url": "https://tyoc213.github.io/blog/xla/fastai/2020/12/13/finding-nemo-a-bug-journey.html",
            "relUrl": "/xla/fastai/2020/12/13/finding-nemo-a-bug-journey.html",
            "date": " • Dec 13, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Compiling xla locally",
            "content": "Introduction . Since the time I found some issues mentioning GPU support https://github.com/pytorch/xla/ I was wondering when I could use it locally because a little group at fastai community have been trying to give support to fastai and hopefully being able to run locally would be useful for that end. . Running GPU support with a docker image . So the first thing was to run it with the docker image which if you have installed container-toolkit/install-guide and then running something like docker run --gpus all -it --shm-size 16G gcr.io/tpu-pytorch/xla@sha256:efe47b7a3875ddfa3ea9c68a12ed5517c19cbb3cf272776fba64bec8a683299f or the sha or nightly you want to run like gcr.io/tpu-pytorch/xla@nightly. . After I ran that, I compiled the source as xla-instructions inside that image and after some hours I could see a &quot;hello&quot; I have made on a cpp file wonderful!!!. But it seems that all that work would be lost on the next startup, so after watch building it and ran successfully I decided to give a go into building on my own computer (if you still need that then maybe check docker commit xxxxx and docker checkpoint). . Compiling locally . To compile locally I have lurked and tested different ways, first time I build it was just with CPU support which I didn&#39;t notice (some env vars where missing), so long history short I have made a new environment with conda like conda create -n xla python=3.6 and worked inside this env. . Installing needed things . Probably I miss something, but I have to install . Don&#39;t use the cuda from apt, use directly from nvidia and install only sdk with sudo sh PATH_CUDA_DRIVERS --silent --toolkit it will be installed to /usr/local/cuda which is where it should be located (if you let Ubuntu handle installation of drivers this --toolkit will not erase that and only install sdk so when updating kernel no need to reinstall). | Install cuddn from NVIDIA from zip file and copy all h files and libs sudo cp cuda/include/cudnn.h /usr/local/cuda/include sudo cp cuda/include/cudnn.h /usr/local/cuda/include sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64 sudo cp cuda/include/cudnn_version.h /usr/local/cuda/include sudo cp cuda/include/cudnn_backend.h /usr/local/cuda/include sudo cp cuda/include/cudnn_adv_infer.h /usr/local/cuda/include sudo cp cuda/include/cudnn_adv_train.h /usr/local/cuda/include sudo cp cuda/include/cudnn_cnn_infer.h /usr/local/cuda/include sudo cp cuda/include/cudnn_cnn_train.h /usr/local/cuda/include sudo cp cuda/include/cudnn_ops_infer.h /usr/local/cuda/include sudo cp cuda/include/cudnn_ops_train.h /usr/local/cuda/include sudo cp cuda/include/cudnn.h /usr/local/cuda/include . | Install sudo apt-get install cmake | Install go to install go get github.com/bazelbuild/bazelisk and then make if you cant run bazel from command line make a ln -s /home/tyoc213/go/bin/bazelisk /home/tyoc213/go/bin/bazel because bazel is needed in the path. | sudo apt-get install clang-8 clang++-8 | pip install lark-parser | conda install -c pytorch magma-cuda110 In my case I have CUDA Version: 11.0 so I used 110 | . Get the sources . git clone --recursive https://github.com/pytorch/pytorch cd pytorch/ git clone --recursive https://github.com/pytorch/xla.git cd xla xla/scripts/apply_patches.sh . The last lines apply xla needed patches. Now you are ready to compile, but wait!!! what is missing is all the configuration that lets you build inside the docker container! . Environment vars . Which are the things I fighted most: . export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&quot;$(dirname $(which conda))/../&quot;} export TF_CUDA_COMPUTE_CAPABILITIES=&quot;7.0,7.5&quot; export CXX_ABI=0 export cxx_abi=0 export GPU_NUM_DEVICES=1 export cuda=1 # new export USE_CUDA=1 export XLA_CUDA=1 export XLA_DEBUG=1 export XLA_BAZEL_VERBOSE=0 export CXX=clang++-8 export CC=clang-8 export GLIBCXX_USE_CXX11_ABI=0 export CFLAGS=&quot;${CFLAGS} -D_GLIBCXX_USE_CXX11_ABI=0&quot; export CXXFLAGS=&quot;${CXXFLAGS} -D_GLIBCXX_USE_CXX11_ABI=0&quot; export PATH=/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/home/tyoc213/go/bin:/home/tyoc213/miniconda3/envs/xla/include:$PATH . Probably some of them are not needed, but this worked out. Also this is not all that is needed, there is one extra set of commands needed because if not the lib will mix CXX11_ABI so it will not link (and you will not know after hours). To apply this inside the pytorch directory: . sed -i &#39;/include(CMakeDependentOption)/i set(GLIBCXX_USE_CXX11_ABI 0)&#39; CMakeLists.txt sed -i &#39;s/set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -std=c++11 -fPIC&quot;)/set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -std=c++11 -fPIC -D_GLIBCXX_USE_CXX11_ABI=0&quot;)/g&#39; third_party/gloo/CMakeLists.txt sed -i &#39;/gloo_list_append_if_unique(CUDA_NVCC_FLAGS &quot;-Xcompiler&quot; &quot;-fPIC&quot;)/i gloo_list_append_if_unique(CUDA_NVCC_FLAGS &quot;-Xcompiler&quot; &quot;-D_GLIBCXX_USE_CXX11_ABI=0&quot;)&#39; third_party/gloo/cmake/Cuda.cmake . Building . So that is all needed if I didn&#39;t miss something. So now we are ready to build this this, start at the top level pytorch: . (xla) tyoc213@u:~/Documents/github/pytorch$ python setup.py install (xla) tyoc213@u:~/Documents/github/pytorch$ cd xla (xla) tyoc213@u:~/Documents/github/pytorch/xla$ python setup.py install . In my 2015 CPU Intel(R) Core(TM) i5-6500 CPU @ 3.20GHz it taked like 2-4 hours compiling pytorch and then 8-10 hours compiling xla (which compiles internally TF). . Finally running . SO now that you have a working xla locally, you need to setup some extra vars to configure XLA for 1 GPU . export XRT_WORKERS=&quot;localservice:0;grpc://localhost:40934&quot; export XRT_DEVICE_MAP=&quot;CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0|GPU:0;/job:localservice/replica:0/task:0/device:XLA_GPU:0&quot; . If you have 4 GPUs, then use export XRT_DEVICE_MAP=&quot;CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0|GPU:0;/job:localservice/replica:0/task:0/device:XLA_GPU:0|GPU:1;/job:localservice/replica:0/task:0/device:XLA_GPU:1|GPU:2;/job:localservice/replica:0/task:0/device:XLA_GPU:2|GPU:3;/job:localservice/replica:0/task:0/device:XLA_GPU:3&quot; . Why All this? . Having xla TPU support is still a missing and wanted feature of fastai, some months a go Butch Landingin and I joined a hackathon to have a little reusable library it worked as a POC and in some moments we did have something working prior fastai 2 release but later we have found &quot;extrange quirks&quot; that have been difficult to track. And lately we have joined forces with Tanishq Mathew Abraham who has been working in his own support for fastai, so hopefully this time we can make this work. . This was the first capture on Nov 26, see how the name says &quot;compute&quot; while on a 2080 . The good parts . the * It also means that we can have XLA tests running without TPU on a GPU and you don&#39;t need to compile, only get latest build and run on docker GPU, or locally with full compiling as explained above. . XLA GPU optimizations could maybe help your current work? and maybe some things can be tested locally before running full production on the cloud. | The operations sended back to run on CPU locally feel not much slow as they are on TPUs just saying that maybe is more expensive to send ops to CPU on TPU that locally, but havent made a lot of tests and this should be only until all the ops are lowered to TPU. | Have all locally allows to change things like you want, for example I can see the slowness of TPU operations inside the fastai loop with chrome://tracing/ modyfing learner and running the XLA-GPU. And have already found a issue haven&#39;t noticed in latest commits. | . The bad parts . I have been only able to step/debug on python code, not on CPP (but hopefully someone that read this knows a tip to check my vscode settings). | maybe I forgot something more specific in these instructions, but if you find an error, please share. | . References . First hint that xla run on GPU GPU support in PyTorch XLA | This week I spammed the guys at xla Running locally which foes first into running with docker, then locally. | The last missing part, the sed error when building pytorch 1.1.0 from source | Most of the build steps are on xla/CONTRIBUTING | .",
            "url": "https://tyoc213.github.io/blog/xla/fastai/2020/11/28/compiling-xla-locally.html",
            "relUrl": "/xla/fastai/2020/11/28/compiling-xla-locally.html",
            "date": " • Nov 28, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "librosa 2015 presentation updated calls",
            "content": "From https://www.youtube.com/watch?v=MhOdbtPhbLU in 2015 . I also copy &amp; paste some captures (they look weird) to see how different it was 5 years a go. . %matplotlib inline import matplotlib.pyplot as plt import seaborn seaborn.set(style=&#39;ticks&#39;) from IPython.display import Audio import numpy as np import scipy import mir_eval import librosa import librosa.display . y,sr = librosa.load(librosa.util.example_audio_file()) . . Important: I removed all the outputs of Audio(data=,rate=) because the notebook was more than 19Mb total . Audio(data=y,rate=sr) # cleaned just to lower size of notebook (run again to see embeded player) . waveform . librosa.display.waveplot(y, sr) . &lt;matplotlib.collections.PolyCollection at 0x7fcb930f68b0&gt; . spectrograms . D = librosa.stft(y) D.shape . (1025, 2647) . log_spectrogram = librosa.power_to_db(D**2, ref=np.max) log_spectrogram.shape . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/core/spectrum.py:1544: UserWarning: power_to_db was called on complex input so phase information will be discarded. To suppress this warning, call power_to_db(np.abs(D)**2) instead. warnings.warn( . (1025, 2647) . librosa.display.specshow(log_spectrogram, x_axis=&#39;time&#39;, y_axis=&#39;linear&#39;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7fcb93009370&gt; . librosa.display.specshow(log_spectrogram, x_axis=&#39;time&#39;, y_axis=&#39;log&#39;) plt.colorbar() . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The &#39;basey&#39; parameter of __init__() has been renamed &#39;base&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The &#39;linthreshy&#39; parameter of __init__() has been renamed &#39;linthresh&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The &#39;linscaley&#39; parameter of __init__() has been renamed &#39;linscale&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) . &lt;matplotlib.colorbar.Colorbar at 0x7fcb92f4e0a0&gt; . Constant q transform . direct log-frecuency analysis? . C = librosa.cqt(y, sr) C.shape . (84, 2647) . librosa.display.specshow(librosa.amplitude_to_db(C**2), x_axis=&#39;time&#39;, y_axis=&#39;cqt_hz&#39;) plt.colorbar() . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/core/spectrum.py:1641: UserWarning: amplitude_to_db was called on complex input so phase information will be discarded. To suppress this warning, call amplitude_to_db(np.abs(S)) instead. warnings.warn( . &lt;matplotlib.colorbar.Colorbar at 0x7fcb92e887c0&gt; . TK: add title . librosa.display.specshow(librosa.amplitude_to_db(C**2, top_db=40), x_axis=&#39;time&#39;, y_axis=&#39;cqt_note&#39;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7fcb92d0f0d0&gt; . TK: add title . Spectral features . Spectral features are often used to analyze harmony or timbre. . Usually the product of a spectrogram and a filter bank. . pitch vs class . CQT measures the energy in each pitch. . Chroma measures the energy in each pitch class. . chroma = librosa.feature.chroma_cqt(C=C, sr=sr) chroma.shape . (12, 2647) . librosa.display.specshow(chroma, x_axis=&#39;time&#39;, y_axis=&#39;chroma&#39;) plt.colorbar() . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:822: UserWarning: Trying to display complex-valued input. Showing magnitude instead. warnings.warn( . &lt;matplotlib.colorbar.Colorbar at 0x7fcb906852b0&gt; . TK: add title . Other spectral features includes MEL spectra, MFCC and Tonnetz . M = librosa.feature.melspectrogram(y=y, sr=sr) MFCC = librosa.feature.mfcc(y=y, sr=sr) tonnetz = librosa.feature.tonnetz(y=y, sr=sr) . Audio effects . y_harmonic, y_percussive = librosa.effects.hpss(y) . Audio(data=y, rate=sr) . Audio(data=y_harmonic, rate=sr) . Audio(data=y_percussive, rate=sr) . plt.figure(figsize=(12,6)) C_harmonic = librosa.cqt(y_harmonic, sr) C_perc = librosa.cqt(y_percussive, sr) plt.subplot(3,1,1), librosa.display.specshow(C**(1./3), y_axis=&#39;cqt_hz&#39;), plt.colorbar() plt.subplot(3,1,2), librosa.display.specshow(C_harmonic**(1./3), y_axis=&#39;cqt_hz&#39;), plt.colorbar() plt.subplot(3,1,3), librosa.display.specshow(C_perc**(1./3), y_axis=&#39;cqt_hz&#39;), plt.colorbar() . (&lt;AxesSubplot:ylabel=&#39;Hz&#39;&gt;, &lt;matplotlib.collections.QuadMesh at 0x7fcb92d88d90&gt;, &lt;matplotlib.colorbar.Colorbar at 0x7fcb92df3a00&gt;) . Onsets and beats . onset_envelope = librosa.onset.onset_strength(y, sr) . onsets = librosa.onset.onset_detect(onset_envelope=onset_envelope) . plt.subplot(2,1,1) plt.plot(onset_envelope, label=&#39;Onset strength&#39;) plt.vlines(onsets, 0, onset_envelope.max(), color=&#39;r&#39;, alpha=0.25, label=&#39;onsets&#39;) plt.xticks([]), plt.yticks([]) plt.legend(frameon=True) plt.axis(&#39;tight&#39;) plt.subplot(2,1,2) librosa.display.waveplot(y, sr) . &lt;matplotlib.collections.PolyCollection at 0x7fcb93b6a220&gt; . onset stregth is used to track beats and estimate tempo . tempo, beats = librosa.beat.beat_track(onset_envelope=onset_envelope) tempo, beats . (129.19921875, array([ 4, 23, 43, 63, 83, 102, 122, 142, 162, 181, 202, 222, 242, 261, 281, 301, 321, 341, 361, 382, 401, 421, 441, 461, 480, 500, 520, 540, 560, 579, 600, 620, 639, 658, 678, 698, 718, 737, 757, 777, 798, 817, 837, 857, 877, 896, 916, 936, 957, 976, 996, 1016, 1036, 1055, 1075, 1095, 1116, 1135, 1155, 1175, 1195, 1214, 1234, 1254, 1275, 1294, 1314, 1334, 1354, 1373, 1393, 1413, 1434, 1453, 1473, 1493, 1513, 1532, 1552, 1572, 1593, 1612, 1632, 1652, 1672, 1691, 1712, 1732, 1752, 1771, 1791, 1811, 1831, 1850, 1870, 1890, 1911, 1931, 1951, 1971, 1990, 2010, 2030, 2050, 2070, 2090, 2110, 2130, 2149, 2169, 2189, 2209, 2229, 2249, 2269, 2288, 2308, 2328, 2348, 2368, 2388, 2408, 2428, 2448, 2467, 2487, 2507, 2527, 2547])) . plt.plot(onset_envelope, label=&#39;Onset strength&#39;) plt.vlines(onsets, 0, onset_envelope.max(), color=&#39;r&#39;, alpha=0.25, label=&#39;onsets&#39;) plt.xticks([]), plt.yticks([]) plt.legend(frameon=True) plt.axis(&#39;tight&#39;) . (-132.3, 2778.3, -0.05, 1.05) . beat events are in frame indices . We can convert to time (in seconds), and sonify with mir_eval . beat_times = librosa.frames_to_time(beats) beat_times . array([ 0.09287982, 0.53405896, 0.99845805, 1.46285714, 1.92725624, 2.36843537, 2.83283447, 3.29723356, 3.76163265, 4.20281179, 4.69043084, 5.15482993, 5.61922902, 6.06040816, 6.52480726, 6.98920635, 7.45360544, 7.91800454, 8.38240363, 8.87002268, 9.31120181, 9.77560091, 10.24 , 10.70439909, 11.14557823, 11.60997732, 12.07437642, 12.53877551, 13.0031746 , 13.44435374, 13.93197279, 14.39637188, 14.83755102, 15.27873016, 15.74312925, 16.20752834, 16.67192744, 17.11310658, 17.57750567, 18.04190476, 18.52952381, 18.97070295, 19.43510204, 19.89950113, 20.36390023, 20.80507937, 21.26947846, 21.73387755, 22.2214966 , 22.66267574, 23.12707483, 23.59147392, 24.05587302, 24.49705215, 24.96145125, 25.42585034, 25.91346939, 26.35464853, 26.81904762, 27.28344671, 27.7478458 , 28.18902494, 28.65342404, 29.11782313, 29.60544218, 30.04662132, 30.51102041, 30.9754195 , 31.43981859, 31.88099773, 32.34539683, 32.80979592, 33.29741497, 33.7385941 , 34.2029932 , 34.66739229, 35.13179138, 35.57297052, 36.03736961, 36.50176871, 36.98938776, 37.43056689, 37.89496599, 38.35936508, 38.82376417, 39.26494331, 39.75256236, 40.21696145, 40.68136054, 41.12253968, 41.58693878, 42.05133787, 42.51573696, 42.9569161 , 43.42131519, 43.88571429, 44.37333333, 44.83773243, 45.30213152, 45.76653061, 46.20770975, 46.67210884, 47.13650794, 47.60090703, 48.06530612, 48.52970522, 48.99410431, 49.4585034 , 49.89968254, 50.36408163, 50.82848073, 51.29287982, 51.75727891, 52.221678 , 52.6860771 , 53.12725624, 53.59165533, 54.05605442, 54.52045351, 54.98485261, 55.4492517 , 55.91365079, 56.37804989, 56.84244898, 57.28362812, 57.74802721, 58.2124263 , 58.6768254 , 59.14122449]) . y_click = mir_eval.sonify.clicks(beat_times, sr, length=len(y)) Audio(data=y+y_click, rate=sr) . Temporal structure . c_sync = librosa.util.sync(chroma, beats, aggregate=np.median) c_sync.shape . (12, 130) . librosa.display.specshow(c_sync, y_axis=&#39;chroma&#39;) plt.colorbar() . /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:822: UserWarning: Trying to display complex-valued input. Showing magnitude instead. warnings.warn( . &lt;matplotlib.colorbar.Colorbar at 0x7fcb904d6100&gt; . history embedding can add context . chroma_stack = librosa.feature.stack_memory(c_sync, n_steps=3, mode=&#39;edge&#39;) chroma_stack.shape . (36, 130) . librosa.display.specshow(chroma_stack, y_axis=&#39;chroma&#39;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7fcb903be0d0&gt; . recurrence plots show nearest neighbor linkage for each frame. . Chroma recurrence can encode harmonic repetitions . # R = librosa.segment.recurrence_matrix(y, sym=True) . #R = librosa.segment.recurrence_matrix(chroma_stack, sym=True) # diagonal lines indicate repeated progressions # librosa.display.specshow(R, aspect=&#39;equal&#39;) # post processing R can reveal structural components, metrical structure, etc . How to plot the different Rs above? .",
            "url": "https://tyoc213.github.io/blog/librosa/audio/2020/09/26/Librosa-2015-presentation-updated-calls.html",
            "relUrl": "/librosa/audio/2020/09/26/Librosa-2015-presentation-updated-calls.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Reconociendo dígitos en fastai2",
            "content": "Instalar fastai2 . http://dev.fast.ai/#Installing yo prefiero instalarlo como dice ahí en su propio entorno de conda. . git clone https://github.com/fastai/fastai2 cd fastai2 pip install -e &quot;.[dev]&quot; . para que CUDA este disponible python -c &#39;import torch; print(torch.cuda.is_available())&#39; si no lo esta checa que este instalado con nvidia-smi, si esta instalado puede que no se haya instalado la versión correcta de pytorch y los drivers instalados ve como hacerlo en https://pytorch.org/get-started/locally/ . Registrandose en kaggle . Registrarse como usuario en kaggle | hacer pip install kaggle | Obtener tu clave privada de usuario y guardarla en la ruta default | Entrar en https://www.kaggle.com/c/digit-recognizer y dar click en aceptar reglas para poder usar la aplicación de kaggle. | Bajar los archivos con kaggle competitions download -c digit-recognizer y descomprimirlos en su propia carpeta | Importar fastai2 . Importamos el modulo vision de fastai2 . from fastai2.vision.all import * . Vamos a definir algunas variables para poder ponerle nombre a nuestros archivos a enviar y guardar el nombre del modelo que usamos. . VERSION = 4 MODELO=resnet34 NOMBRE_MODELO=&quot;resnet34&quot; BASE_FILE = f&quot;base-{NOMBRE_MODELO}_v{VERSION}&quot; SUBMIT_FILE = f&quot;submit-{NOMBRE_MODELO}_v{VERSION}&quot; FINE_FILE = f&quot;base-{NOMBRE_MODELO}-fine_v{VERSION}&quot; SUBMIT_FINE_FILE = f&quot;submit-{NOMBRE_MODELO}-fine_v{VERSION}&quot; . Fastai2 trabaja muy bien con data loaders, estos necesitan que exista algun archivo en disco, lo cual puede ayudar para datasets grandes, pero tal vez no mucho a su lectura recurrente. Sólo necesitamos preprocesar esto la primer vez, por lo mismo usamos esos ifs para extraerlos si es necesario desde cada row del csv hacia la imagen en disco. . import string # in the same folder I have downloaded with: kaggle competitions download -c digit-recognizer path = untar_data(&quot;file://digit-recognizer.zip&quot;, dest=&quot;.&quot;) #print(path) #print(path.ls()) from PIL import Image from matplotlib import cm import pandas as pd path = Path(&quot;digit-recognizer&quot;) df = pd.read_csv(path/&quot;train.csv&quot;, header=&#39;infer&#39;) df.head() . label pixel0 pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ... pixel774 pixel775 pixel776 pixel777 pixel778 pixel779 pixel780 pixel781 pixel782 pixel783 . 0 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 785 columns . %%time def get_image(csv_row): img = csv_row.reshape(28,28) x = cm.gist_earth(img)*255 return Image.fromarray(np.uint8(x)) def explode_train(): df = pd.read_csv(path/&#39;train.csv&#39;, header=&#39;infer&#39;) imagenes = df.iloc[:,1:].apply(lambda x: x.values, axis=1).values labels = df.iloc[:,:1].apply(lambda x: x.values[0], axis=1) p = Path(path)/&#39;train&#39; p.mkdir(parents=True, exist_ok=True) for idx, l in enumerate(labels): im = get_image(imagenes[idx]) image_name = (str(l)+&quot;_&quot;+&#39;train&#39;+&#39;_&#39;+&#39;&#39;.join(random.choice(string.ascii_lowercase) for i in range(8))) + &quot;.png&quot; im.save(p/image_name) def explode_test(): df = pd.read_csv(path/&#39;test.csv&#39;, header=&#39;infer&#39;) imagenes = df.iloc[:,:].apply(lambda x: x.values, axis=1).values p = Path(path)/&#39;test&#39; p.mkdir(parents=True, exist_ok=True) for idx, l in enumerate(imagenes): im = get_image(imagenes[idx]) im.save(p / f&quot;{idx}.png&quot;) if not (path/&#39;train&#39;).exists(): explode_train() if not (path/&#39;test&#39;).exists(): explode_test() . CPU times: user 106 µs, sys: 13 µs, total: 119 µs Wall time: 81.1 µs . Para leer todas las imagenes generadas, lo haciemos por medio de un DataBlock el cual es un blueprint de donde, como y que vamos a hacer con esas imagenes para obtener finalmente las imagenes en batches o grupos de imagenes (de 64 en 64 por default) y la carga de las imagenes como tal se hace por medio de data_loader = data_block.dataloaders(path) el cual en este caso también aprende el &quot;vocabulario&quot; a aprender ya que se ha especificado que el segundo bloque es un bloque de categorias a diferencia del primero que es de Imagenes. . get_my_labels obtiene a partir de el archivo de imagen leído el nombre y regresa el dígito al que pertenece del vocabulario. . %%time def get_my_labels(fname): return int(fname.name[0]) dblock = DataBlock( splitter = RandomSplitter(), item_tfms = Resize(224), blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, get_y = get_my_labels ) dls = dblock.dataloaders(&quot;digit-recognizer/train&quot;) dls.vocab . CPU times: user 4.12 s, sys: 549 ms, total: 4.67 s Wall time: 4.81 s . (#10) [0,1,2,3,4,5,6,7,8,9] . show batch . Podemos ver que realmente este encontrando las imagenes mostrando un batch de imagenes. . dls.show_batch() . Cargar un modelo y entrenarlo . En fastai2 podemos cargar un modelo ya entrenado y reusarlo para ajustarlo a la tarea que queremos llevar a cabo. . learn = cnn_learner(dls, MODELO, metrics=error_rate) . Nuestro modelo en este momento ya tiene una estructura la cual se puede sumarizar así . learn.summary() . Sequential (Input shape: [&#39;64 x 3 x 224 x 224&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 64 x 112 x 112 9,408 False ________________________________________________________________ BatchNorm2d 64 x 64 x 112 x 112 128 True ________________________________________________________________ ReLU 64 x 64 x 112 x 112 0 False ________________________________________________________________ MaxPool2d 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 73,728 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 8,192 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 294,912 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 32,768 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 1,179,648 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 131,072 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 512 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 512 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 1024 0 False ________________________________________________________________ BatchNorm1d 64 x 1024 2,048 True ________________________________________________________________ Dropout 64 x 1024 0 False ________________________________________________________________ Linear 64 x 512 524,288 True ________________________________________________________________ ReLU 64 x 512 0 False ________________________________________________________________ BatchNorm1d 64 x 512 1,024 True ________________________________________________________________ Dropout 64 x 512 0 False ________________________________________________________________ Linear 64 x 10 5,120 True ________________________________________________________________ Total params: 21,817,152 Total trainable params: 549,504 Total non-trainable params: 21,267,648 Optimizer used: &lt;function Adam at 0x7fb52353ab00&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group number 2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Podemos ver las predicciones actuales antes de reajustar los parametros de la red. En rojo se muestran los errores en verde los que estan bien. Los aciertos más bien son aleatorios. . Los resultados como se espera no pueden reconocer los patrones ya que esta inicializada para otro tipo de tarea. . learn.show_results() . Ahora podemos entrenarlo con fine_tune . El método fit sirve para ejecutar una serie de pasadas sobre los datos de entrenamiento. . %%time learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.227946 | 0.135213 | 0.040000 | 01:16 | . epoch train_loss valid_loss error_rate time . 0 | 0.035625 | 0.025032 | 0.007024 | 01:43 | . CPU times: user 2min 15s, sys: 26.2 s, total: 2min 42s Wall time: 3min . learn.show_results() . Nuestro modelo en este momento ya puede asertar con más confianza el dígito que se le esta pasando . learn.predict(Path(&quot;digit-recognizer&quot;)/&#39;test&#39;/&quot;1111.png&quot;) . (&#39;2&#39;, tensor(2), tensor([8.4126e-10, 8.9289e-08, 1.0000e+00, 7.2959e-07, 2.4069e-09, 2.0494e-08, 2.9449e-09, 1.0405e-07, 2.0946e-07, 1.2205e-10])) . Si volvemos a checar los resultados podemos ver que ciertamente ya puede reconocer los patrones. . Veamos cuales son las imagenes que más costaron reconocer. . interp = Interpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(15,10)) . Para guardar los resultados obtenidos y mandarlos a kaggle, basta con hacer lo siguiente . def predict_test(the_learner, file_name): p = path/&#39;test&#39; print(f&quot;predicting {len(p.ls())} in {p}&quot;) l = [] for idx, img in enumerate(p.ls()): fname = p/f&quot;{idx}.png&quot; pred = the_learner.predict(fname) # la predicción contiene todo el resultado, la predicción esta en el elemento 0 l.append( [idx+1, int(pred[0])] ) if idx % 2800 == 0: print(f&quot;{[idx, pred[0]]}...&quot;) df = pd.DataFrame(l) h = [&quot;ImageId&quot;,&quot;label&quot;] df.to_csv(f&quot;{file_name}.csv&quot;, header=h, index=False) if True else print(&quot;** skipped save **&quot;) print(&quot;done!&quot;) . Removemos el callback del loader . %%time learn.remove_cb(learn.cbs[2]) predict_test(learn, BASE_FILE) . predicting 28000 in digit-recognizer/test [0, &#39;2&#39;]... [2800, &#39;4&#39;]... [5600, &#39;8&#39;]... [8400, &#39;5&#39;]... [11200, &#39;4&#39;]... [14000, &#39;3&#39;]... [16800, &#39;1&#39;]... [19600, &#39;5&#39;]... [22400, &#39;9&#39;]... [25200, &#39;3&#39;]... done! CPU times: user 53min 39s, sys: 1h 36min 21s, total: 2h 30min Wall time: 2h 49min 33s . El Learner que actualmente se encuentra en learn es muy bueno para quedar cerca de los primeros mil competidores, lo que tenemos que hacer ahora es mejorarlo poco a poco. . Pero antes de esto vamos a salvarlo de 2 formas: . exportandolo el cual requiere que despues se vuelva a cargar y crear otro cnn | salvar y cargar de manera directa | . Guardamos el modelo actual para poder cargarlo despues via load. . learn.save(BASE_FILE) if True else print(&quot;No se a guardado el archivo&quot;) . Fine tunning . Ahora se puede cargar desde donde se quedo el paso guardado por save anterior, pero debe de contener datos a los cuales referirse, por eso se carga a travez de un modelo instanciado igual que antes, sólo que este ya esta &quot;entrenado&quot; a la tarea actual. . l2 = cnn_learner(dls, MODELO, metrics=error_rate) . Una vez instanciado igual que el modelo que guardamos, se puede cargar el mismo desde el archivo que se guardo con learn.save . l2.load(BASE_FILE) . &lt;fastai2.learner.Learner at 0x7fb5202f10d0&gt; . Buscamos un buen learning rate . l2.lr_find() . SuggestedLRs(lr_min=0.00043651582673192023, lr_steep=6.309573450380412e-07) . Descongelamos el modelo entrenado para que se pueda entrenar nuevamente y usamos el learning rate que encontramos anteriormente . %%time l2.unfreeze() l2.fine_tune(8, 1e-2) . epoch train_loss valid_loss error_rate time . 0 | 0.080212 | 0.057889 | 0.010238 | 01:10 | . epoch train_loss valid_loss error_rate time . 0 | 0.074369 | 0.067521 | 0.013690 | 01:36 | . 1 | 0.082539 | 0.081573 | 0.018690 | 01:36 | . 2 | 0.058147 | 0.052700 | 0.015238 | 01:35 | . 3 | 0.042055 | 0.044290 | 0.009048 | 01:34 | . 4 | 0.029418 | 0.040308 | 0.009048 | 01:33 | . 5 | 0.008509 | 0.022320 | 0.004286 | 01:34 | . 6 | 0.003744 | 0.021935 | 0.004286 | 01:36 | . 7 | 0.000918 | 0.023328 | 0.004524 | 01:35 | . CPU times: user 10min 46s, sys: 2min 52s, total: 13min 38s Wall time: 13min 53s . Guardamos el nuevo modelo ajustado . l2.save(FINE_FILE) . Y lo usamos para predecir eliminando también el callback del progress bar . l2.cbs, l2.cbs[2] . ((#3) [TrainEvalCallback,Recorder,ProgressCallback], ProgressCallback) . %%time l2.remove_cb(l2.cbs[2]) predict_test(l2, FINE_FILE) . predicting 28000 in digit-recognizer/test [0, &#39;2&#39;]... [2800, &#39;4&#39;]... [5600, &#39;8&#39;]... [8400, &#39;5&#39;]... [11200, &#39;4&#39;]... [14000, &#39;3&#39;]... [16800, &#39;1&#39;]... [19600, &#39;5&#39;]... [22400, &#39;9&#39;]... [25200, &#39;3&#39;]... done! CPU times: user 39min 42s, sys: 1h 35min 30s, total: 2h 15min 13s Wall time: 2h 36min 45s . l2.validate() . (#2) [0.023327725008130074,0.0045238095335662365] . Exportar a producci&#243;n . Si no queremos seguir reentrenando nuestro trabajo, podemos sólo exportar el modelo para que se use en predicciones al cargarlo. . learn.export(BASE_FILE) if False else print(&quot;no se ha exportado&quot;) . no se ha exportado . l2.export(FINE_FILE) if False else print(&quot;no se ha exportado nada&quot;) . no se ha exportado nada .",
            "url": "https://tyoc213.github.io/blog/fastai2/2020/05/26/Reconocer-d%C3%ADgitos.html",
            "relUrl": "/fastai2/2020/05/26/Reconocer-d%C3%ADgitos.html",
            "date": " • May 26, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ":) .",
          "url": "https://tyoc213.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tyoc213.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}