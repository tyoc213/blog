[
  {
    "objectID": "posts/2020-12-13-finding nemo a bug journey.html",
    "href": "posts/2020-12-13-finding nemo a bug journey.html",
    "title": "Finding nemo a bug journey",
    "section": "",
    "text": "The major part of the last months we were trying to solve one bug that we didn’t know how to solve until this days, some time before fastai version 2 release having a POC and passing the hackathon we participated we left a little the project, but when we came back to it, there were some extrange things happening, some models where not training and apparently others where training, we didn’t understand what happened, in that time we thought we introduced an error in our code some way.\n\n\nSo, these last few days since I had locally installed XLA and been able to run things, I planned to take a new round to find our bug and it was a perfect opportunity to test what having locally installed pytorch+xla+fatai_xla_etensions could do.\nSo I passed a lot of assumptions to finally find the solution.\n\nSo after having locally installed XLA, one of the first things I wanted to test is if locally I could reproduce the error, and I could!\nThe first one was that the error was in our code, but having locally XLA allowed me to test the exact same code changing the device to either: CPU, CUDA or XLA. So I ended up having 2 data loaders and 2 trains on the same python file. Also one main point was that I wrapped a Adam but from pytorch with OptimWrapper and it trained correctly so I was more suspicious of differences between the optimizer from fastai and the native to pytorch because there is one know difference about __getstate__ that is also a requirement for TPU Pods.\nIn the past we have also thought that it was a freeze unfreeze problem, but it was also discarded, so this time I was checking the optimizer, but could not find why the params were not training even when looking under the lens.\nBut after more testings and so on, I see that the second example started to train correctly while the first on the file not, and it was that with all the fresh runs, so I thought it was a problem with the learner, but could not find a “real problem” so I returned back to the optimizer and all, but this time I have a new “tool” I learned, counting the trainable parameters so, the trainable parameters their gradients are updated when you call backward, so I started to count there and for the first example they where always zero while for the second run since start, they have a number. So the next task was to find why on the first the trainable parameters are always zero and the second not.\n\nBut I still didn’t get why one model was training and the other one not!\nI passed _BaseOptimizer, Optimizer, Learner and others and still could not find the problem, so I decided to compare models and found the problem! I updated the example I found in pytorch forums https://discuss.pytorch.org/t/two-models-with-same-weights-different-results/8918/7 the original one at first run did break because it compared tensors not on same device, so it threw error, I modified it so that it prints them nicely instead of being caught in that error.\ndef compare_models(model_1, model_2):\n    models_differ = 0\n    for key_item_1, key_item_2 in zip(model_1.state_dict().items(), model_2.state_dict().items()):\n        if key_item_1[1].device == key_item_2[1].device  and torch.equal(key_item_1[1], key_item_2[1]):\n            pass\n        else:\n            models_differ += 1\n            if (key_item_1[0] == key_item_2[0]):\n                _device = f'device {key_item_1[1].device}, {key_item_2[1].device}' if key_item_1[1].device != key_item_2[1].device else ''\n                print(f'Mismatch {_device} found at', key_item_1[0])\n            else:\n                raise Exception\n    if models_differ == 0:\n        print('Models match perfectly! :)')\nAnd that was the solution to the problem, I focused on seeing why the models parameters were on different devices. At the end I have something like (remember I don’t need to patch the optimizer because I have all installed locally).\ndef create_opt(self):\n    print('trainable count before', len(self.all_params(with_grad=True)))\n    self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)\n    print('trainable count after', len(self.all_params(with_grad=True)))\n    if not self.wd_bn_bias:\n        for p in self._bn_bias_state(True ): p['do_wd'] = False\n    if self.train_bn:\n        for p in self._bn_bias_state(False): p['force_train'] = True\nand\n        print('trainable count before backward', len(self.all_params(with_grad=True)))\n        self('before_backward')\n        print('trainable count before backward', len(self.all_params(with_grad=True)))\n        self._backward()\n        self('after_backward')\nSo at the end I see that even that the model is moved later to the device, the first time when splitter=trainable_params in self.opt = self.opt_func(self.splitter(self.model), lr=self.lr) inside create_opt it is not there, so the parameters where stuck on CPU while the the data and the model is later moved to the XLA device.\nThis does not affects GPUs, but thinking about it could mean also something about the pickable behaviour of xla tensors, specially the optimizer, but that is an history for another time, right now, we have again a simple lib that works for Single-device TPUs where you need to modify zero code from fastai.\n\n\n\nSo the model need to be in TPU/XLA device before their parameters are taked by splitter on Optimizer initialization, I guess we assumed some things in between then and now. At the end it was not exactly an error but was. But sure it was difficult to track, now knowing what it is is solved and we can continue forward.\nI hope to add in the next release a show_lowering_ops (or similar) to print the counters if you have hit some of those and it is easy to print in a model that runs with this activated. The MNIST demo should be working again don’t forget to peek at fastai_xla_extensions.\nEXTRA NOTE: But there was error because XLA model on CPU not trained when updating backward from data operations on XLA device? well, now I think that XLA worked on TPU with model and data copied to TPU on first time but somehow our model got stuck on CPU so not trained, it became another model separate from the execution happening on TPU (I can think of pickable things, but that is unknown at the moment)"
  },
  {
    "objectID": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html",
    "href": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html",
    "title": "Dos and don’ts with invitation to make study groups",
    "section": "",
    "text": "I started learning deep learning 2 or 3 years ago watching the Jeremy videos. I learned some things but I didn’t code, watching videos isn’t enough, sure I learned to use Jupyter lab and were able to follow some tutorials and understand some concepts but still lots of things were still vague.\nMy first study group ever was for reading the fastabook, were we joined the meeting read silently the chapter in turn and then try to answer 1 or 2 questions that were on the lines of why or how and don’t focus on what questions. It took some months to read 18 chapters (one chapter per week). As all study groups, it started with more people than the ones who could finish it. It was my first time finishing a technical book in quite some time and feel that I have learned and appreciated things I could not do only watching videos.\nBy the end last months of the 2020 I have already think of taking a look at walk with fastai 2020 course but I was “slow” and not finding time to do it, was only a “target” or a TODO item. So after I joined a meeting on the voice channel of fastai discord, I had time to talk with Zachary and ask him if it was “fine” to start a new study group for his course and he liked the idea. So the first thing I did was to watch the videos fast forward and make the timings for youtube at same time Zach was working on updating and moving all to this new site with a submenu and all the work needed to keep up to date working notebooks.\nWhen I posted the invitation for this study group, I chose to make the study group meet 2 times a week: one to watch the video and the next one to execute the notebook(s) for the class. I think this is hard because people need to keep 2 slots of their week time, but what I was trying to make is like make the watch the video “optional” and run it yourself not optional (so you can still watch the video on your own)."
  },
  {
    "objectID": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#tools",
    "href": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#tools",
    "title": "Dos and don’ts with invitation to make study groups",
    "section": "tools",
    "text": "tools\n\nInstead of using the video sharing on discord, I decided to use metastream to share the playback of the video (I also had at the side a copy of the “blank” times of the video to skip easely)\nFor the firsts sessions I used locally jupyter lab and VSCode (which I also showed how I worked with nbdev and VSCode hand in hand), but after some updates on local drivers and pytorch don’t providing support for the newest drivers I ended using colab (you can only have 2 running notebooks on colab).\nDiscord was used to talk/discuss the video and most of all in the second session to run your own notebook."
  },
  {
    "objectID": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#first-weeks",
    "href": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#first-weeks",
    "title": "Dos and don’ts with invitation to make study groups",
    "section": "First weeks",
    "text": "First weeks\nFor the first meetings more than 10 people joined, they installed the required plugin by metastream and watched the shared play it was a nice start and for the “run the notebook meeting” I decided not to take a look more than the video itself and did not run the notebook before hand so the first meeting I was only able to cover the first pet session instead of the 3 notebooks and I used 1:30 hours (some people left because as I have said sometimes time constraints are difficult).\nIn that presentation I showed how to use Jupyter, how to see where the code is from visual studio code and if I’m not wrong a little about how to work on nbdev and vscode one side or the other. I also skip a little and showed how to call show training loop and maybe other things that were explained later in the videos.\nAlso it was the first time I was getting feedback from participants, while I have been a user of fastai I was not able to completely understand the progress bar and some one hinted to me that it shows the current accuracy, how many samples of the batch are and the remaining time over runtime. Also got feedback to understand the recorder plot learning rate, all these things I have seen in videos and ran in notebooks before, but didn’t understand the little details."
  },
  {
    "objectID": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#middle-sessions",
    "href": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#middle-sessions",
    "title": "Dos and don’ts with invitation to make study groups",
    "section": "middle sessions",
    "text": "middle sessions\nWatching the video together was mostly just that but on Thursday for me it still took quite some time to finish just one notebook, still the discussions were nice and extended, always learning something from the people who shared their knowledge or taking more internally something I have seen before.\nI started to see more of the patterns of the library as I explained them and got more used to seeing the common parts of the high level API, and understood a little better the difference between having and instance of a dataloader taht is used to pass data to the learner in batches and the “template” needed to make the instance which is a DataBlock.\nAlso at some point people started to show less and finally the people that ended where the ones showing up meet by meet.\nI have also received the feedback that I should run notebooks beforehand to see if there are possible errors instead of tackling them at the moment and with this I could start covering more than 1 notebook in 1 hour because sometimes running a specific line and “fixing” the issue at the moment which sometimes result in orienting the talk to other issues instead of the notebook itself."
  },
  {
    "objectID": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#final-sessions",
    "href": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#final-sessions",
    "title": "Dos and don’ts with invitation to make study groups",
    "section": "final sessions",
    "text": "final sessions\nAs with all study groups (and maybe a lot of things in life) at end there were a lot less people the people that committed till the end, probably just because making time to have a schedule on your own time is not easy to keep, but also could be my bad handling of the group, or that 2 meetings a week is a lot, or maybe I didn’t provide enough good information and was boring. But as other people have said, don’t worry about that, is what happens with these study groups and lots of things that surge out of pure interest.\nFrom time to time people more knowledgeable showed up not only the ones attending since the start and shared some of their knowledge, that is always appreciated in any meeting.\nAnd finally the last sessions took place and we finally did it from start to finish, and for me it was a great experience being the first time doing something like this, learning from people mostly, it was also a way to go over the course, because as I have said I only have think to go over the course for some time until I did it this way.\nNice things that happened was that Zachary also joined some times and molly (which leaded the first study group I joined) showed up and explained more advanced things to fellows, things that I we haven’t thought about or knew how to explain, so it was nice."
  },
  {
    "objectID": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#dos",
    "href": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#dos",
    "title": "Dos and don’ts with invitation to make study groups",
    "section": "Do’s",
    "text": "Do’s\n\nRecord the meetings there will be always useful information for newcomers or even to middle or advanced users (sadly I didn’t do this thinking that people will be mad if I recorded the meetings, but while running the study group, people started to ask about recordings).\nDo run notebooks beforehand so that errors pop out if any exist and solve it beforehand (you can explain them in session and some of them could be as easy as downloading an image). Also being ready to present and is not “cheating”. Is being ready to explain ;).\nWrite a blogpost for each meeting or keep track of each meeting in a note, basically keep track of the things you did learn or understand better after the meeting (is like sending notes after a meeting just to check that those points are important). I didn’t keep track but learned a lot of things. Sadly this is the only blog post I have made about these meetings, but I’m sure I will forget some things if I don’t put them in practice soon or write about them. This tip can also help people watching the streams or course.\nBe honest about what you don’t know or understand, there are people that have the knowledge or people that understand better or in a different way so invite them to share the knowledge.\nBe clear about dates/times, sadly the time we did our study group it didn’t fit people in Europe (3am for them), but here is where recordings could help.\nFocus on a level, or make it clear, for me this was more beginner friendly, but interactions with people make it advanced or more helpful than just beginner friendly (and it is always good that someone with more experience joins).\n\nOn that note, if you have something that you can’t answer and there is no one around that can explain it, feel free to try to ping friends on chat or someone on the chat at the moment so that they can join at the moment (or answer later in text).\n\nSubmit bugs or changes to the repo you are using for reference so that it keeps up to date, the maintainer will like help with maintaining source in good shape to be helpful as it was intended when created.\nEven if you don’t understand something, having or attending a study group will expose you to the opportunity to exchange other people’s point of view and understanding of the subject. As someone pointed in one meeting “I have been working with pandas the whole time, but found 1 call on the notebooks that I didn’t know, so it is always good to watch other people how they use a tool you always discover something you aren’t using”.\nIf possible have a target to apply what all are learning in the study group, one of the things I could not solve was the invitation or wondering of people asking if we will do a kaggle competition, analyse something, in other way people were asking “how we can apply this we are learning now?”, sure in the future what has been learned will contribute to solve something, but for this time I didn’t have a project, kaggle comp or something to give to the attendees.\nTry to make questions that ask for: why and how, questions that can move you or others to learn furter."
  },
  {
    "objectID": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#donts",
    "href": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#donts",
    "title": "Dos and don’ts with invitation to make study groups",
    "section": "Dont’s",
    "text": "Dont’s\n\nNot keep track of what you learned, not just read, but make a note on what you understand now that didn’t before, make your own resume.\nAs organizer don’t forget about the meeting and if you will not have time because something show up do one of this:\n\nnotify on time (at least 30 min before and if possible at start of the day or even before that).\nask if someone wants to lead a meeting (hopefully within a week from the event), you can invite your own attendees probably making it their first time leading an online meeting. I didn’t do this, but it would be nice to invite people to take the lead on something if they want to participate like that.\n\nSay things that you will not meet on time.\nImprove if possible what you are following, but I have sended one PR to Zach, there were a lot of little PRs that I wanted to do as improvements or extra hints but haven’t done."
  },
  {
    "objectID": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#maybe-donts",
    "href": "posts/2021-04-10-dos-and-donts-with-invitation-to-make-study-groups.html#maybe-donts",
    "title": "Dos and don’ts with invitation to make study groups",
    "section": "Maybe dont’s",
    "text": "Maybe dont’s\nMaybe doing 2 meetings a week is too much, I’m not sure, but maybe ask or make clear that one of the meetings they can do on their own and the other is more participative. Also I have seen some study groups have 2 meetings to tackle bigger books in “little time”."
  },
  {
    "objectID": "posts/2021-01-02-reconociendo-una-buena-idea-de-investigacion.html",
    "href": "posts/2021-01-02-reconociendo-una-buena-idea-de-investigacion.html",
    "title": "¿Cómo reconocer las buenas ideas de investigación que valen tu tiempo y esfuerzo?",
    "section": "",
    "text": "How do you recognize good research ideas that are worth your time and effort? And how do you evaluate whether the hypotheses of others make sense? Here is my take:1/ pic.twitter.com/LKy8INNCRm\n\n— Cecile Janssens (@cecilejanssens) February 24, 2020\n\n\nRecordando que el método científico\n\n\n\nMétodo científico\n\n\nHay una parte que es inductiva (se proponen hipotesis) y otra que es deductiva (se comprueba rigurosamente la conclusión a partir de las hipótesis).\n\n\nDoing science means following the scientific method. The hypotheses we choose to investigate (also in hypothesis-free research) follows from the rigor of our background research and a process that is called inductive reasoning. pic.twitter.com/ojK18UTVK2\n\n— Cecile Janssens (@cecilejanssens) February 24, 2020\n\n\nLa parte deductiva incluye: hacer una pregunta, investigar, hacer una hipotesis. La parte inductiva incluye: construir un experimiento para probar la hipotesis, analisar los datos/resultados y sacar una conclusión\n\n\nThis case can be strong or weak. The case is cogent if we can rely on the premises to be true. A case can be made stronger with more supporting evidence from more background research, and weaker e.g., if new evidence contradicts. pic.twitter.com/dz4ncNTFX5\n\n— Cecile Janssens (@cecilejanssens) February 24, 2020\n\n\nEntonces, las premisas por lo menos antes de empezar a trabajar deberían de parecer validas (si no se tienen las pruebas) o en otras palabras ser “solidas” (“sound” en Inglés).\n\n\nHere are examples of evidence that are essential to justify that the diet might work (also the study protocol paper that they cite did not provide this evidence). With a weak case, an RCT is more likely to show no benefit of the intervention (the diet didn't work). pic.twitter.com/q8iiBKjofx\n\n— Cecile Janssens (@cecilejanssens) February 24, 2020\n\n\nTambien si basas tu trabajo en otros que no proveen pruebas, talvez sea indicación de poco trabajo previo o mal trabajo previo.\nEntonces, se debe buscar tener un fuerte argumento a favor o por lo menos una corazonada fuerte de que al final del camino existe la razon para ocupar tu tiempo en ese argumento respecto a la pregunta inicial."
  },
  {
    "objectID": "posts/2020-05-26-reconocer-dígitos.html",
    "href": "posts/2020-05-26-reconocer-dígitos.html",
    "title": "Reconociendo dígitos en fastai2",
    "section": "",
    "text": "http://dev.fast.ai/#Installing yo prefiero instalarlo como dice ahí en su propio entorno de conda.\ngit clone https://github.com/fastai/fastai2\ncd fastai2\npip install -e \".[dev]\"\npara que CUDA este disponible python -c 'import torch; print(torch.cuda.is_available())' si no lo esta checa que este instalado con nvidia-smi, si esta instalado puede que no se haya instalado la versión correcta de pytorch y los drivers instalados ve como hacerlo en https://pytorch.org/get-started/locally/\n\n\n\nRegistrarse como usuario en kaggle\nhacer pip install kaggle\nObtener tu clave privada de usuario y guardarla en la ruta default\nEntrar en https://www.kaggle.com/c/digit-recognizer y dar click en aceptar reglas para poder usar la aplicación de kaggle.\nBajar los archivos con kaggle competitions download -c digit-recognizer y descomprimirlos en su propia carpeta\n\n\n\n\nImportamos el modulo vision de fastai2\n\nfrom fastai2.vision.all import *\n\nVamos a definir algunas variables para poder ponerle nombre a nuestros archivos a enviar y guardar el nombre del modelo que usamos.\n\nVERSION = 4\nMODELO=resnet34\nNOMBRE_MODELO=\"resnet34\"\nBASE_FILE = f\"base-{NOMBRE_MODELO}_v{VERSION}\"\nSUBMIT_FILE = f\"submit-{NOMBRE_MODELO}_v{VERSION}\"\n\nFINE_FILE = f\"base-{NOMBRE_MODELO}-fine_v{VERSION}\"\nSUBMIT_FINE_FILE = f\"submit-{NOMBRE_MODELO}-fine_v{VERSION}\"\n\nFastai2 trabaja muy bien con data loaders, estos necesitan que exista algun archivo en disco, lo cual puede ayudar para datasets grandes, pero tal vez no mucho a su lectura recurrente. Sólo necesitamos preprocesar esto la primer vez, por lo mismo usamos esos ifs para extraerlos si es necesario desde cada row del csv hacia la imagen en disco.\n\nimport string\n# in the same folder I have downloaded with: kaggle competitions download -c digit-recognizer\npath = untar_data(\"file://digit-recognizer.zip\", dest=\".\")\n#print(path)\n#print(path.ls())\n\nfrom PIL import Image\nfrom matplotlib import cm\n\nimport pandas as pd\n\npath = Path(\"digit-recognizer\")\ndf = pd.read_csv(path/\"train.csv\", header='infer')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      label\n      pixel0\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      ...\n      pixel774\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 785 columns\n\n\n\n\n%%time\ndef get_image(csv_row):\n    img = csv_row.reshape(28,28)\n    x = cm.gist_earth(img)*255\n    return Image.fromarray(np.uint8(x))\n\ndef explode_train():\n    df = pd.read_csv(path/'train.csv', header='infer')\n    imagenes = df.iloc[:,1:].apply(lambda x: x.values, axis=1).values\n    labels = df.iloc[:,:1].apply(lambda x: x.values[0], axis=1)\n    p = Path(path)/'train'\n    p.mkdir(parents=True, exist_ok=True)\n        \n    for idx, l in enumerate(labels):\n        im = get_image(imagenes[idx])\n        image_name = (str(l)+\"_\"+'train'+'_'+''.join(random.choice(string.ascii_lowercase) for i in range(8))) + \".png\"\n        im.save(p/image_name)\n\ndef explode_test():\n    df = pd.read_csv(path/'test.csv', header='infer')\n    imagenes = df.iloc[:,:].apply(lambda x: x.values, axis=1).values\n    p = Path(path)/'test'\n    p.mkdir(parents=True, exist_ok=True)\n    for idx, l in enumerate(imagenes):\n        im = get_image(imagenes[idx])\n        im.save(p / f\"{idx}.png\")\nif not (path/'train').exists():\n    explode_train()\nif not (path/'test').exists():\n    explode_test()\n\nCPU times: user 106 µs, sys: 13 µs, total: 119 µs\nWall time: 81.1 µs\n\n\nPara leer todas las imagenes generadas, lo haciemos por medio de un DataBlock el cual es un blueprint de donde, como y que vamos a hacer con esas imagenes para obtener finalmente las imagenes en batches o grupos de imagenes (de 64 en 64 por default) y la carga de las imagenes como tal se hace por medio de data_loader = data_block.dataloaders(path) el cual en este caso también aprende el “vocabulario” a aprender ya que se ha especificado que el segundo bloque es un bloque de categorias a diferencia del primero que es de Imagenes.\nget_my_labels obtiene a partir de el archivo de imagen leído el nombre y regresa el dígito al que pertenece del vocabulario.\n\n%%time\ndef get_my_labels(fname):\n    return int(fname.name[0])\n\ndblock = DataBlock(\n    splitter  = RandomSplitter(),\n    item_tfms = Resize(224),\n    blocks    = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y     = get_my_labels\n    )\n\ndls = dblock.dataloaders(\"digit-recognizer/train\")\ndls.vocab\n\nCPU times: user 4.12 s, sys: 549 ms, total: 4.67 s\nWall time: 4.81 s\n\n\n(#10) [0,1,2,3,4,5,6,7,8,9]\n\n\n\n\n\nPodemos ver que realmente este encontrando las imagenes mostrando un batch de imagenes.\n\ndls.show_batch()\n\n\n\n\n\n\n\nEn fastai2 podemos cargar un modelo ya entrenado y reusarlo para ajustarlo a la tarea que queremos llevar a cabo.\n\nlearn = cnn_learner(dls, MODELO, metrics=error_rate)\n\nNuestro modelo en este momento ya tiene una estructura la cual se puede sumarizar así\n\nlearn.summary()\n\nSequential (Input shape: ['64 x 3 x 224 x 224'])\n================================================================\nLayer (type)         Output Shape         Param #    Trainable \n================================================================\nConv2d               64 x 64 x 112 x 112  9,408      False     \n________________________________________________________________\nBatchNorm2d          64 x 64 x 112 x 112  128        True      \n________________________________________________________________\nReLU                 64 x 64 x 112 x 112  0          False     \n________________________________________________________________\nMaxPool2d            64 x 64 x 56 x 56    0          False     \n________________________________________________________________\nConv2d               64 x 64 x 56 x 56    36,864     False     \n________________________________________________________________\nBatchNorm2d          64 x 64 x 56 x 56    128        True      \n________________________________________________________________\nReLU                 64 x 64 x 56 x 56    0          False     \n________________________________________________________________\nConv2d               64 x 64 x 56 x 56    36,864     False     \n________________________________________________________________\nBatchNorm2d          64 x 64 x 56 x 56    128        True      \n________________________________________________________________\nConv2d               64 x 64 x 56 x 56    36,864     False     \n________________________________________________________________\nBatchNorm2d          64 x 64 x 56 x 56    128        True      \n________________________________________________________________\nReLU                 64 x 64 x 56 x 56    0          False     \n________________________________________________________________\nConv2d               64 x 64 x 56 x 56    36,864     False     \n________________________________________________________________\nBatchNorm2d          64 x 64 x 56 x 56    128        True      \n________________________________________________________________\nConv2d               64 x 64 x 56 x 56    36,864     False     \n________________________________________________________________\nBatchNorm2d          64 x 64 x 56 x 56    128        True      \n________________________________________________________________\nReLU                 64 x 64 x 56 x 56    0          False     \n________________________________________________________________\nConv2d               64 x 64 x 56 x 56    36,864     False     \n________________________________________________________________\nBatchNorm2d          64 x 64 x 56 x 56    128        True      \n________________________________________________________________\nConv2d               64 x 128 x 28 x 28   73,728     False     \n________________________________________________________________\nBatchNorm2d          64 x 128 x 28 x 28   256        True      \n________________________________________________________________\nReLU                 64 x 128 x 28 x 28   0          False     \n________________________________________________________________\nConv2d               64 x 128 x 28 x 28   147,456    False     \n________________________________________________________________\nBatchNorm2d          64 x 128 x 28 x 28   256        True      \n________________________________________________________________\nConv2d               64 x 128 x 28 x 28   8,192      False     \n________________________________________________________________\nBatchNorm2d          64 x 128 x 28 x 28   256        True      \n________________________________________________________________\nConv2d               64 x 128 x 28 x 28   147,456    False     \n________________________________________________________________\nBatchNorm2d          64 x 128 x 28 x 28   256        True      \n________________________________________________________________\nReLU                 64 x 128 x 28 x 28   0          False     \n________________________________________________________________\nConv2d               64 x 128 x 28 x 28   147,456    False     \n________________________________________________________________\nBatchNorm2d          64 x 128 x 28 x 28   256        True      \n________________________________________________________________\nConv2d               64 x 128 x 28 x 28   147,456    False     \n________________________________________________________________\nBatchNorm2d          64 x 128 x 28 x 28   256        True      \n________________________________________________________________\nReLU                 64 x 128 x 28 x 28   0          False     \n________________________________________________________________\nConv2d               64 x 128 x 28 x 28   147,456    False     \n________________________________________________________________\nBatchNorm2d          64 x 128 x 28 x 28   256        True      \n________________________________________________________________\nConv2d               64 x 128 x 28 x 28   147,456    False     \n________________________________________________________________\nBatchNorm2d          64 x 128 x 28 x 28   256        True      \n________________________________________________________________\nReLU                 64 x 128 x 28 x 28   0          False     \n________________________________________________________________\nConv2d               64 x 128 x 28 x 28   147,456    False     \n________________________________________________________________\nBatchNorm2d          64 x 128 x 28 x 28   256        True      \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   294,912    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nReLU                 64 x 256 x 14 x 14   0          False     \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   32,768     False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nReLU                 64 x 256 x 14 x 14   0          False     \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nReLU                 64 x 256 x 14 x 14   0          False     \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nReLU                 64 x 256 x 14 x 14   0          False     \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nReLU                 64 x 256 x 14 x 14   0          False     \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nReLU                 64 x 256 x 14 x 14   0          False     \n________________________________________________________________\nConv2d               64 x 256 x 14 x 14   589,824    False     \n________________________________________________________________\nBatchNorm2d          64 x 256 x 14 x 14   512        True      \n________________________________________________________________\nConv2d               64 x 512 x 7 x 7     1,179,648  False     \n________________________________________________________________\nBatchNorm2d          64 x 512 x 7 x 7     1,024      True      \n________________________________________________________________\nReLU                 64 x 512 x 7 x 7     0          False     \n________________________________________________________________\nConv2d               64 x 512 x 7 x 7     2,359,296  False     \n________________________________________________________________\nBatchNorm2d          64 x 512 x 7 x 7     1,024      True      \n________________________________________________________________\nConv2d               64 x 512 x 7 x 7     131,072    False     \n________________________________________________________________\nBatchNorm2d          64 x 512 x 7 x 7     1,024      True      \n________________________________________________________________\nConv2d               64 x 512 x 7 x 7     2,359,296  False     \n________________________________________________________________\nBatchNorm2d          64 x 512 x 7 x 7     1,024      True      \n________________________________________________________________\nReLU                 64 x 512 x 7 x 7     0          False     \n________________________________________________________________\nConv2d               64 x 512 x 7 x 7     2,359,296  False     \n________________________________________________________________\nBatchNorm2d          64 x 512 x 7 x 7     1,024      True      \n________________________________________________________________\nConv2d               64 x 512 x 7 x 7     2,359,296  False     \n________________________________________________________________\nBatchNorm2d          64 x 512 x 7 x 7     1,024      True      \n________________________________________________________________\nReLU                 64 x 512 x 7 x 7     0          False     \n________________________________________________________________\nConv2d               64 x 512 x 7 x 7     2,359,296  False     \n________________________________________________________________\nBatchNorm2d          64 x 512 x 7 x 7     1,024      True      \n________________________________________________________________\nAdaptiveAvgPool2d    64 x 512 x 1 x 1     0          False     \n________________________________________________________________\nAdaptiveMaxPool2d    64 x 512 x 1 x 1     0          False     \n________________________________________________________________\nFlatten              64 x 1024            0          False     \n________________________________________________________________\nBatchNorm1d          64 x 1024            2,048      True      \n________________________________________________________________\nDropout              64 x 1024            0          False     \n________________________________________________________________\nLinear               64 x 512             524,288    True      \n________________________________________________________________\nReLU                 64 x 512             0          False     \n________________________________________________________________\nBatchNorm1d          64 x 512             1,024      True      \n________________________________________________________________\nDropout              64 x 512             0          False     \n________________________________________________________________\nLinear               64 x 10              5,120      True      \n________________________________________________________________\n\nTotal params: 21,817,152\nTotal trainable params: 549,504\nTotal non-trainable params: 21,267,648\n\nOptimizer used: <function Adam at 0x7fb52353ab00>\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nModel frozen up to parameter group number 2\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback\n\n\nPodemos ver las predicciones actuales antes de reajustar los parametros de la red. En rojo se muestran los errores en verde los que estan bien. Los aciertos más bien son aleatorios.\nLos resultados como se espera no pueden reconocer los patrones ya que esta inicializada para otro tipo de tarea.\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\nEl método fit sirve para ejecutar una serie de pasadas sobre los datos de entrenamiento.\n\n%%time\nlearn.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.227946\n      0.135213\n      0.040000\n      01:16\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.035625\n      0.025032\n      0.007024\n      01:43\n    \n  \n\n\n\nCPU times: user 2min 15s, sys: 26.2 s, total: 2min 42s\nWall time: 3min\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\nNuestro modelo en este momento ya puede asertar con más confianza el dígito que se le esta pasando\n\nlearn.predict(Path(\"digit-recognizer\")/'test'/\"1111.png\")\n\n\n\n\n('2',\n tensor(2),\n tensor([8.4126e-10, 8.9289e-08, 1.0000e+00, 7.2959e-07, 2.4069e-09, 2.0494e-08,\n         2.9449e-09, 1.0405e-07, 2.0946e-07, 1.2205e-10]))\n\n\nSi volvemos a checar los resultados podemos ver que ciertamente ya puede reconocer los patrones.\nVeamos cuales son las imagenes que más costaron reconocer.\n\ninterp = Interpretation.from_learner(learn)\ninterp.plot_top_losses(9, figsize=(15,10))\n\n\n\n\n\n\n\nPara guardar los resultados obtenidos y mandarlos a kaggle, basta con hacer lo siguiente\n\ndef predict_test(the_learner, file_name):\n    p = path/'test'\n    print(f\"predicting {len(p.ls())} in {p}\")\n    l = []\n    for idx, img in enumerate(p.ls()):\n        fname = p/f\"{idx}.png\"\n        pred = the_learner.predict(fname)\n        # la predicción contiene todo el resultado, la predicción esta en el elemento 0\n        l.append( [idx+1, int(pred[0])] )\n        if idx % 2800 == 0:\n            print(f\"{[idx, pred[0]]}...\")\n    df = pd.DataFrame(l)\n    h = [\"ImageId\",\"label\"]\n    df.to_csv(f\"{file_name}.csv\", header=h, index=False) if True else print(\"** skipped save **\")\n    print(\"done!\")\n\nRemovemos el callback del loader\n\n%%time\nlearn.remove_cb(learn.cbs[2])\npredict_test(learn, BASE_FILE)\n\npredicting 28000 in digit-recognizer/test\n[0, '2']...\n[2800, '4']...\n[5600, '8']...\n[8400, '5']...\n[11200, '4']...\n[14000, '3']...\n[16800, '1']...\n[19600, '5']...\n[22400, '9']...\n[25200, '3']...\ndone!\nCPU times: user 53min 39s, sys: 1h 36min 21s, total: 2h 30min\nWall time: 2h 49min 33s\n\n\nEl Learner que actualmente se encuentra en learn es muy bueno para quedar cerca de los primeros mil competidores, lo que tenemos que hacer ahora es mejorarlo poco a poco.\nPero antes de esto vamos a salvarlo de 2 formas:\n\nexportandolo el cual requiere que despues se vuelva a cargar y crear otro cnn\nsalvar y cargar de manera directa\n\nGuardamos el modelo actual para poder cargarlo despues via load.\n\nlearn.save(BASE_FILE) if True else print(\"No se a guardado el archivo\")\n\n\n\n\nAhora se puede cargar desde donde se quedo el paso guardado por save anterior, pero debe de contener datos a los cuales referirse, por eso se carga a travez de un modelo instanciado igual que antes, sólo que este ya esta “entrenado” a la tarea actual.\n\nl2 = cnn_learner(dls, MODELO, metrics=error_rate)\n\nUna vez instanciado igual que el modelo que guardamos, se puede cargar el mismo desde el archivo que se guardo con learn.save\n\nl2.load(BASE_FILE)\n\n<fastai2.learner.Learner at 0x7fb5202f10d0>\n\n\nBuscamos un buen learning rate\n\nl2.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.00043651582673192023, lr_steep=6.309573450380412e-07)\n\n\n\n\n\nDescongelamos el modelo entrenado para que se pueda entrenar nuevamente y usamos el learning rate que encontramos anteriormente\n\n%%time\nl2.unfreeze()\nl2.fine_tune(8, 1e-2) \n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.080212\n      0.057889\n      0.010238\n      01:10\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.074369\n      0.067521\n      0.013690\n      01:36\n    \n    \n      1\n      0.082539\n      0.081573\n      0.018690\n      01:36\n    \n    \n      2\n      0.058147\n      0.052700\n      0.015238\n      01:35\n    \n    \n      3\n      0.042055\n      0.044290\n      0.009048\n      01:34\n    \n    \n      4\n      0.029418\n      0.040308\n      0.009048\n      01:33\n    \n    \n      5\n      0.008509\n      0.022320\n      0.004286\n      01:34\n    \n    \n      6\n      0.003744\n      0.021935\n      0.004286\n      01:36\n    \n    \n      7\n      0.000918\n      0.023328\n      0.004524\n      01:35\n    \n  \n\n\n\nCPU times: user 10min 46s, sys: 2min 52s, total: 13min 38s\nWall time: 13min 53s\n\n\nGuardamos el nuevo modelo ajustado\n\nl2.save(FINE_FILE)\n\nY lo usamos para predecir eliminando también el callback del progress bar\n\nl2.cbs, l2.cbs[2]\n\n((#3) [TrainEvalCallback,Recorder,ProgressCallback], ProgressCallback)\n\n\n\n%%time\nl2.remove_cb(l2.cbs[2])\npredict_test(l2, FINE_FILE)\n\npredicting 28000 in digit-recognizer/test\n[0, '2']...\n[2800, '4']...\n[5600, '8']...\n[8400, '5']...\n[11200, '4']...\n[14000, '3']...\n[16800, '1']...\n[19600, '5']...\n[22400, '9']...\n[25200, '3']...\ndone!\nCPU times: user 39min 42s, sys: 1h 35min 30s, total: 2h 15min 13s\nWall time: 2h 36min 45s\n\n\n\nl2.validate()\n\n(#2) [0.023327725008130074,0.0045238095335662365]\n\n\n\n\n\nSi no queremos seguir reentrenando nuestro trabajo, podemos sólo exportar el modelo para que se use en predicciones al cargarlo.\n\nlearn.export(BASE_FILE) if False else print(\"no se ha exportado\")\n\nno se ha exportado\n\n\n\nl2.export(FINE_FILE) if False else print(\"no se ha exportado nada\")\n\nno se ha exportado nada"
  },
  {
    "objectID": "posts/2020-11-28-compiling-xla-locally.html",
    "href": "posts/2020-11-28-compiling-xla-locally.html",
    "title": "Compiling xla locally",
    "section": "",
    "text": "Since the time I found some issues mentioning GPU support https://github.com/pytorch/xla/ I was wondering when I could use it locally because a little group at fastai community have been trying to give support to fastai and hopefully being able to run locally would be useful for that end."
  },
  {
    "objectID": "posts/2020-11-28-compiling-xla-locally.html#installing-needed-things",
    "href": "posts/2020-11-28-compiling-xla-locally.html#installing-needed-things",
    "title": "Compiling xla locally",
    "section": "Installing needed things",
    "text": "Installing needed things\nProbably I miss something, but I have to install\n\nDon’t use the cuda from apt, use directly from nvidia and install only sdk with sudo sh PATH_CUDA_DRIVERS --silent --toolkit it will be installed to /usr/local/cuda which is where it should be located (if you let Ubuntu handle installation of drivers this --toolkit will not erase that and only install sdk so when updating kernel no need to reinstall).\nInstall cuddn from NVIDIA from zip file and copy all h files and libs\n\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\nsudo cp cuda/include/cudnn_version.h /usr/local/cuda/include\nsudo cp cuda/include/cudnn_backend.h /usr/local/cuda/include\nsudo cp cuda/include/cudnn_adv_infer.h /usr/local/cuda/include\nsudo cp cuda/include/cudnn_adv_train.h /usr/local/cuda/include\nsudo cp cuda/include/cudnn_cnn_infer.h /usr/local/cuda/include\nsudo cp cuda/include/cudnn_cnn_train.h /usr/local/cuda/include\nsudo cp cuda/include/cudnn_ops_infer.h /usr/local/cuda/include\nsudo cp cuda/include/cudnn_ops_train.h /usr/local/cuda/include\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\n\nInstall sudo apt-get install cmake\nInstall go to install go get github.com/bazelbuild/bazelisk and then make if you cant run bazel from command line make a ln -s /home/tyoc213/go/bin/bazelisk /home/tyoc213/go/bin/bazel because bazel is needed in the path.\nsudo apt-get install clang-8 clang++-8\npip install lark-parser\nconda install -c pytorch magma-cuda110 In my case I have CUDA Version: 11.0 so I used 110"
  },
  {
    "objectID": "posts/2020-11-28-compiling-xla-locally.html#get-the-sources",
    "href": "posts/2020-11-28-compiling-xla-locally.html#get-the-sources",
    "title": "Compiling xla locally",
    "section": "Get the sources",
    "text": "Get the sources\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch/\ngit clone --recursive https://github.com/pytorch/xla.git\ncd xla\nxla/scripts/apply_patches.sh\nThe last lines apply xla needed patches. Now you are ready to compile, but wait!!! what is missing is all the configuration that lets you build inside the docker container!"
  },
  {
    "objectID": "posts/2020-11-28-compiling-xla-locally.html#environment-vars",
    "href": "posts/2020-11-28-compiling-xla-locally.html#environment-vars",
    "title": "Compiling xla locally",
    "section": "Environment vars",
    "text": "Environment vars\nWhich are the things I fighted most:\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\nexport TF_CUDA_COMPUTE_CAPABILITIES=\"7.0,7.5\"\nexport CXX_ABI=0\nexport cxx_abi=0\nexport GPU_NUM_DEVICES=1\nexport cuda=1 # new\nexport USE_CUDA=1\nexport XLA_CUDA=1\nexport XLA_DEBUG=1\nexport XLA_BAZEL_VERBOSE=0\nexport CXX=clang++-8\nexport CC=clang-8\nexport GLIBCXX_USE_CXX11_ABI=0\nexport CFLAGS=\"${CFLAGS} -D_GLIBCXX_USE_CXX11_ABI=0\"\nexport CXXFLAGS=\"${CXXFLAGS} -D_GLIBCXX_USE_CXX11_ABI=0\"\nexport PATH=/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/home/tyoc213/go/bin:/home/tyoc213/miniconda3/envs/xla/include:$PATH\nProbably some of them are not needed, but this worked out. Also this is not all that is needed, there is one extra set of commands needed because if not the lib will mix CXX11_ABI so it will not link (and you will not know after hours). To apply this inside the pytorch directory:\nsed -i '/include(CMakeDependentOption)/i set(GLIBCXX_USE_CXX11_ABI 0)' CMakeLists.txt\nsed -i 's/set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -fPIC\")/set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -fPIC -D_GLIBCXX_USE_CXX11_ABI=0\")/g' third_party/gloo/CMakeLists.txt\nsed -i '/gloo_list_append_if_unique(CUDA_NVCC_FLAGS \"-Xcompiler\" \"-fPIC\")/i gloo_list_append_if_unique(CUDA_NVCC_FLAGS \"-Xcompiler\" \"-D_GLIBCXX_USE_CXX11_ABI=0\")' third_party/gloo/cmake/Cuda.cmake"
  },
  {
    "objectID": "posts/2020-11-28-compiling-xla-locally.html#building",
    "href": "posts/2020-11-28-compiling-xla-locally.html#building",
    "title": "Compiling xla locally",
    "section": "Building",
    "text": "Building\nSo that is all needed if I didn’t miss something. So now we are ready to build this this, start at the top level pytorch:\n(xla) tyoc213@u:~/Documents/github/pytorch$ python setup.py install\n(xla) tyoc213@u:~/Documents/github/pytorch$ cd xla\n(xla) tyoc213@u:~/Documents/github/pytorch/xla$ python setup.py install\nIn my 2015 CPU Intel(R) Core(TM) i5-6500 CPU @ 3.20GHz it taked like 2-4 hours compiling pytorch and then 8-10 hours compiling xla (which compiles internally TF)."
  },
  {
    "objectID": "posts/2020-11-28-compiling-xla-locally.html#finally-running",
    "href": "posts/2020-11-28-compiling-xla-locally.html#finally-running",
    "title": "Compiling xla locally",
    "section": "Finally running",
    "text": "Finally running\nSO now that you have a working xla locally, you need to setup some extra vars to configure XLA for 1 GPU\nexport XRT_WORKERS=\"localservice:0;grpc://localhost:40934\"\nexport XRT_DEVICE_MAP=\"CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0|GPU:0;/job:localservice/replica:0/task:0/device:XLA_GPU:0\"\nIf you have 4 GPUs, then use export XRT_DEVICE_MAP=\"CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0|GPU:0;/job:localservice/replica:0/task:0/device:XLA_GPU:0|GPU:1;/job:localservice/replica:0/task:0/device:XLA_GPU:1|GPU:2;/job:localservice/replica:0/task:0/device:XLA_GPU:2|GPU:3;/job:localservice/replica:0/task:0/device:XLA_GPU:3\""
  },
  {
    "objectID": "posts/2020-11-28-compiling-xla-locally.html#the-good-parts",
    "href": "posts/2020-11-28-compiling-xla-locally.html#the-good-parts",
    "title": "Compiling xla locally",
    "section": "The good parts",
    "text": "The good parts\nthe * It also means that we can have XLA tests running without TPU on a GPU and you don’t need to compile, only get latest build and run on docker GPU, or locally with full compiling as explained above. * XLA GPU optimizations could maybe help your current work? and maybe some things can be tested locally before running full production on the cloud. * The operations sended back to run on CPU locally feel not much slow as they are on TPUs just saying that maybe is more expensive to send ops to CPU on TPU that locally, but havent made a lot of tests and this should be only until all the ops are lowered to TPU. * Have all locally allows to change things like you want, for example I can see the slowness of TPU operations inside the fastai loop with chrome://tracing/ modyfing learner and running the XLA-GPU. And have already found a issue haven’t noticed in latest commits."
  },
  {
    "objectID": "posts/2020-11-28-compiling-xla-locally.html#the-bad-parts",
    "href": "posts/2020-11-28-compiling-xla-locally.html#the-bad-parts",
    "title": "Compiling xla locally",
    "section": "The bad parts",
    "text": "The bad parts\n\nI have been only able to step/debug on python code, not on CPP (but hopefully someone that read this knows a tip to check my vscode settings).\nmaybe I forgot something more specific in these instructions, but if you find an error, please share."
  },
  {
    "objectID": "posts/2021-01-09-fastchai-rbracco-audio-y-estudio-propio.html",
    "href": "posts/2021-01-09-fastchai-rbracco-audio-y-estudio-propio.html",
    "title": "Audio y auto estudio interview",
    "section": "",
    "text": "Here's my interview with Robbert Bracco @MadeUpMasters (author of \"Things Jeremy Says to do\" on the @fastdotai forums) all about deep learning and https://t.co/wANZD5nFcn (library) applied to audio, self-study in ML.Audio: https://t.co/T9CeIBtUKrVideo: https://t.co/ZCJzuxCIwG\n\n— Sanyam Bhutani (@bhutanisanyam1) September 1, 2019\n\n\n\nParte 1\nhttps://youtu.be/k-gZAyg5ib8?t=0 Introducción\n\nhttps://twitter.com/madeupmasters\nhttps://forums.fast.ai/t/things-jeremy-says-to-do/36682\n\n“Para lograr ser autodidacta se requiere ser consistente y un desafío balanceado”.\nY hay que ver en teoría en una escuela “quieren que construyas un compilador antes antes de hacer una pieza útil de software”\n“La AI puede ser más flexible para detectar cambios en un audio que la programación tradicional” lo cual podría ser complicado de programar de manera imperativa o funcional.\ntomó los cursos * https://cs50.harvard.edu/college/2021/spring/ ó https://online-learning.harvard.edu/course/cs50-introduction-computer-science para principiantes en computación y amplio * https://www.coursera.org/learn/algorithms-part1 * https://www.coursera.org/learn/algorithms-part2 y probablemente sirva de algo el código de este libro https://algs4.cs.princeton.edu/home/ * https://www.coursera.org/learn/machine-learning by Andrew Ng\n“focus on building not on theory” “top-down approach”, en otras palabras es más practico hacer cosas que entender el trasfondo que puede ser más complejo y requerir de más análisis.\n\n\nparte 2\nhttps://youtu.be/k-gZAyg5ib8?t=986 brincar a una competencia sin saber nada te da la libertad de experimentar lo que se te ocurra. Un punto interesante es que si tienes suficientes bases puedes explorar más libremente que sabiendo las herramientas default usadas o los modelos usados por defecto “un periodo de creatividad libre”, una de las estrategias que tomó en ese tiempo fue pasar la onda de audio directo al modelo y posteriormente se dió cuenta que tendría que conocer más acerca del procesamiento de señales para poder sacar más información del audio.\n“It is easy to get addicted to online classes” se puede entrar en un “ciclo infinito” de aprender que puede bloquear el de aplicar o hacer algo.\nhttps://hackernoon.com/how-not-to-do-fast-ai-or-any-ml-mooc-3d34a7e0ab8c\n“What should I don next? is to see what you have in your head and see what is stopping you, then learn that”, enforcarse en aprender sólo las cosas que te faltan por aprender es mejor, es como el eslabon más débil de la cadena siempre es el que se puede romper, hay que aprender a hacer objetivos basados en los “bloqueos” que nos encontramos en el camino.\nKaggle te puede aislar hacia resolver el problema a la mano en vez de toparte con lo que es construir tu propio proyecto porque ya tienes de alguna forma los datos procesados y un objetivo, pero si no lo tienes todo de inicio puede resultar en otra forma de aprendizaje (¿talvez no es posible?, ¿los datos afectan?). Es mejor si se pueden hacer las dos cosas Kaggle+proyecto(s) propio(s).\nhttp://christinemcleavey.com/musenet/\n\n\nparte 3\nhttps://youtu.be/k-gZAyg5ib8?t=1356\nEl objetivo de fastai_audio es contruir un modulo que sea compatible con fastai con la misma usabilidad.\nResnets y densenets parecen funcionar bien con FFTs y audio. Y se puede usar transfer learning desde imagenette aunque no tenga nada que ver con audio, probablemente porque las primeras capas reconocen líneas, direcciones y otras características básicas.\nY a base de prueba y error los defaults en fastai_audio han sido puestos en la librería.\nY en cuando a hacer cosas, es mejor hacerlas aunque tengan errores o no sean perfectas (o incluso aunque no las entiendas), talvez alguien más pueda ver el error y corregirlo. Si se espera a ser experto en el area antes de empezar algo, pues por lo menos falta llegar a ese punto primero sin hacer nada práctico de antemano. Y es bueno tener a alguién o algun recurso en quien confiar ya sea una persona, un foro en general alguién a quien poder preguntar aunque tal vez no tengan todas las respuestas.\n(SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition][https://arxiv.org/abs/1904.08779] y https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\n\n\nparte 4\nhttps://youtu.be/k-gZAyg5ib8?t=2730\n“consistency matters above all” la consistencia importa más que cualquier otra cosa, poner esfuerzo constante con días de descanso es mejor que hacer mucho en este momento, dejarlo, volver y que sobre esforzarse al punto de “quemarte”. “Intellectual work is exhausting” el trabajo intelectual es agotador, “I’m working from 8:00 a.m. to 2:00 p.m.” trabajo de las 8:00 a.m. a las 2:00 p.m. Con un tiempo más límitado tienes que enfocarte en hacer ciertas cosas y tomar decisiones en ese tiempo sin tener todo el tiempo del día para cosas “fribolas”."
  },
  {
    "objectID": "posts/2021-06-05-lineas_de_fraude.html",
    "href": "posts/2021-06-05-lineas_de_fraude.html",
    "title": "Curvando la línea para llevar un fraude",
    "section": "",
    "text": "Referencias a videos originales\nEl EVITEMOS EL FRAUDE !! APRENDE A ANALIZAR EL PREP DEL INE CON PYTHON !!! y las partes describiendo el código estan en\n\n0:0:0-24:50 (peque pandas code para plotear lineas de diferencia de votos)\n1:27:50-1:29:08 (como se ve un fraude)\n\nDesgraciadamente el INE decidio quitar la página inicial del PREP, pero por fortuna no quitaron a donde apunta y se puede encontrar una copia en https://web.archive.org/web/20210526174841/https://www.ine.mx/voto-y-elecciones/prep/ que la ultima disponible es el 27 de Mayo del 2021.\nFinalmente para descargar el mismo archivo que se muestra en el video navegar aquí y descargarlo localmente https://prep2018.ine.mx/#/diputaciones/nacional/1/3/1/1\n\nEste otro video explica más a fondo como se pueden detectar los fraudes más a fondo\n¡NO TE DEJES ENGAÑAR! MODELOS MATEMÁTICOS PARA DETECTAR FRAUDES ELECTORALES\n\n\nEjecutando en colab\nEste notebook usa python y se puede ejecutar en la nube por cualquier persona.\nLo siguiente que tenemos que hacer es subirlo, lo unico que tienes que hacer es ejecutarlo mediante el menu\n\nRuntime -> Run all\n\nó\n\nEntorno de ejecución -> Ejecutar todas\n\nó simplemente mediante CTRL+F9\nAl subirlo mediante el siguiente formulario debería de mostarse algo similar a esto \n\nfrom google.colab import files\nuploaded = files.upload()\n\n\n%%time\n# descomprimir el archivo que hayas subido desde el prep\n!unzip 20180702_2100_PREP.zip\n\nArchive:  20180702_2100_PREP.zip\n extracting: 20180702_2100_PREP_diputaciones.zip  \n extracting: 20180702_2100_PREP_presidencia.zip  \n extracting: 20180702_2100_PREP_senadurias.zip  \nCPU times: user 7.46 ms, sys: 872 µs, total: 8.33 ms\nWall time: 273 ms\n\n\n\n!unzip 20180702_2100_PREP_diputaciones.zip\n\nArchive:  20180702_2100_PREP_diputaciones.zip\n   creating: 20180702_2100_PREP_diputaciones/\n  inflating: 20180702_2100_PREP_diputaciones/diputaciones.csv  \n  inflating: 20180702_2100_PREP_diputaciones/diputaciones_candidaturas_2018.csv  \n  inflating: 20180702_2100_PREP_diputaciones/LEEME.txt  \n\n\nSi ya haz descomprimido los archivos, a partir de aquí puedes sacar las cuentas cumulativas entre un partido y el resto de partidos\n\n# cargar las librerías necesarias\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rc('figure', figsize=(10, 10))\n\n\n# definimos algunos valores de las variables\n\npartidos = ['MORENA', 'PAN', 'PRI', 'PRD']\ntiempo='FECHA_HORA_CAPTURA'\n\n\n# leemos los datos\ndatos = pd.read_csv('20180702_2100_PREP_diputaciones/diputaciones.csv', sep='|', engine='python', header=5)\n\n\n# vamos a descartar las líneas que no se contabilizaron\nlineas_malas = datos[datos['CONTABILIZADA']==0].index\ndatos.drop(lineas_malas, inplace=True)\n\n\ndef limpia(data, columna):\n    data[columna] = data[columna].replace('-', 0)\n    data[columna] = data[columna].replace('Ilegible', 0)\n    data[columna] = data[columna].replace('Sin dato', 0)\n    data[columna] = pd.to_numeric(data[columna], errors='raise')\n\n\nfor partido in partidos:\n    limpia(datos, partido)\n\n\ndatos.sample(5)\n\n\n\n\n\n  \n    \n      \n      CLAVE_CASILLA\n      CLAVE_ACTA\n      ID_ESTADO\n      ESTADO\n      ID_DISTRITO_FEDERAL\n      DISTRITO_FEDERAL\n      SECCION\n      ID_CASILLA\n      TIPO_CASILLA\n      EXT_CONTIGUA\n      ...\n      CONTABILIZADA\n      MECANISMOS_TRASLADO\n      SHA\n      FECHA_HORA_ACOPIO\n      FECHA_HORA_CAPTURA\n      FECHA_HORA_VERIFICACION\n      ORIGEN\n      DIGITALIZACION\n      TIPO_DOCUMENTO\n      COTEJADA\n    \n  \n  \n    \n      1373\n      '010263C0100'\n      '010263C010006'\n      1\n      AGUASCALIENTES\n      3\n      Aguascalientes\n      0263\n      1\n      C\n      0\n      ...\n      1\n      D\n      4ab896849e13fcc2a26b1962115c2041d23bff9e8bee0b...\n      2018-07-01 22:28:00\n      2018-07-01 23:06:00\n      2018-07-01 23:08:02\n      CATD\n      ESC�NER\n      Acta PREP\n      1\n    \n    \n      15969\n      '071735C0200'\n      '071735C020006'\n      7\n      CHIAPAS\n      6\n      Tuxtla Guti�rrez\n      1735\n      2\n      C\n      0\n      ...\n      0\n      D\n      -\n      -\n      -\n      -\n      CATD\n      ESC�NER\n      -\n      -\n    \n    \n      142531\n      '301356B0100'\n      '301356B010006'\n      30\n      VERACRUZ\n      2\n      Tantoyuca\n      1356\n      1\n      B\n      0\n      ...\n      1\n      D\n      c153534dc09dee6577bdf41e45204db25ac05237b3ed04...\n      2018-07-02 08:27:00\n      2018-07-02 09:22:00\n      2018-07-02 09:23:27\n      CATD\n      ESC�NER\n      Acta PREP\n      1\n    \n    \n      98989\n      '191505C0200'\n      '191505C020006'\n      19\n      NUEVO LE�N\n      6\n      Monterrey\n      1505\n      2\n      C\n      0\n      ...\n      1\n      D\n      1f48bb9db93215c4db2a46c73a7459122e9b6956fa4a81...\n      2018-07-02 16:26:00\n      2018-07-02 16:45:00\n      2018-07-02 16:46:18\n      CATD\n      ESC�NER\n      Acta PREP\n      -\n    \n    \n      150484\n      '304531C0100'\n      '304531C010006'\n      30\n      VERACRUZ\n      16\n      C�rdoba\n      4531\n      1\n      C\n      0\n      ...\n      1\n      F-D\n      2f7475f3513f0981216175beb202322a5bb37ca524ad65...\n      2018-07-01 21:56:00\n      2018-07-01 23:03:00\n      2018-07-01 23:29:15\n      CASILLA\n      M�VIL\n      Acta PREP\n      1\n    \n  \n\n5 rows × 56 columns\n\n\n\n\n# ordenamos los datos por tiempo (la columna 'FECHA_HORA_CAPTURA')\ndatos_ordenados = datos.sort_values(by=[tiempo])\ndatos_ordenados.reset_index(inplace=True)\n\n\ndiferencias = []\nfor partido in partidos[1:]:\n    base = partidos[0]\n    contra = partido\n    print(base, 'vs', partido)\n    resta = np.cumsum(datos_ordenados[base]-datos_ordenados[contra])\n    diferencias.append(resta)\n\n\nfor d in diferencias:\n    plt.plot(d)\nplt.ylabel('diferencia de votos')\nplt.show()\n\nMORENA vs PAN\nMORENA vs PRI\nMORENA vs PRD\n\n\n\n\n\n\n\nDudas o comentarios\nAquí abajo puedes dejar tus comentarios o dudas que tengas para ejecutar tu mismo este análisis y talvez después otros si más gente coopera al conocimiento colectivo, ¡vamos que es fácil compartir!, recuerden que las cosas que no se coparten se pierden."
  },
  {
    "objectID": "posts/2021-01-24-op_lowering_pythorchXLA.html",
    "href": "posts/2021-01-24-op_lowering_pythorchXLA.html",
    "title": "Lowering SiLU in pytorch/XLA",
    "section": "",
    "text": "As a follow up of compiling pytorch locally, the next objective was to lower an operation, but the documentation on pytorch and XLA is almost the same (think of XLA as an extension of Pytorch), you can check the OP_LOWERING_GUIDE which basically is OP LOWERING GUIDE from pytorch, probably you will get it at first hand, but I was not exactly sure what to do next for example I didn’t know when to modify gen.py (which you don’t need if it is already in see and other things.\n\n\nNow that I have lowered an op, I think what the lowering guide says is something like this:\n\nThat you need to have an environment to compile/test/run\nThat you need to understand the operation SiLU\nThat you need to implement new operations copying verbatim the signature from header and cpp files to aten_xla_type.h/cpp from aten_xla_type_default.h/cpp\nYou will use XLATensor tensors as input and output as pytorch tensor (ATen means “a tensor library”)\nFrom this level in tensor_methods will be ir ops that are constructed from pytorch to use the tensorflow XLA compiler ops (which I think it is not exhaustive? not all is output to the doc?). This operations include for example the * I used and xla::Sigmoid\n\nThey also list 2 Add inverse op and test and Implement the lowering for HardSigmoid and HardSigmoidBackward which could help and are a good start too."
  },
  {
    "objectID": "posts/2021-01-24-op_lowering_pythorchXLA.html#create-the-base",
    "href": "posts/2021-01-24-op_lowering_pythorchXLA.html#create-the-base",
    "title": "Lowering SiLU in pytorch/XLA",
    "section": "1. Create the base",
    "text": "1. Create the base\nFirst commit https://github.com/pytorch/xla/pull/2721/commits/c16fedbbee3662d3470629dc7fff51c63dd60855\nIt provides the base and starting point:\n\nCopy the signature from the header and implementation of to ``\nCopy the body from the header to .\nIt also reused at a higher level the Sigmoid as expected, the problem with this is that the generated graph for the compiler will list this as a x * sigmoid(x) (which was basically this input.GetIrValue() * ir::ops::Sigmoid(input.GetIrValue());) instead of a SiLU in the node graph.\n\nBut this implementation was good enough to compile without errors and actually run my rudimentary base test that output the same values for cpu implementation and XLA implementation\nimport torch\nfrom torch.nn import SiLU\nimport torch_xla.core.xla_model as xm\n\n\ndede=xm.xla_device()\nm = SiLU()\nm = m.to(dede)\n\ninput = torch.randn(2)\ninput2 = input.clone() # this is on CPU\ninput = input.to(dede)\n\noutput = m(input)\nprint(output)\nprint(output.device) # should print xla\n\nm2 = SiLU()\nprint(\"normal\")\nprint('input2', input2)\no2 = m2(input2)\nprint(o2) # this should match print above\nThe review of the PR suggested the next step, which is:"
  },
  {
    "objectID": "posts/2021-01-24-op_lowering_pythorchXLA.html#go-deeper-with-the-lowering",
    "href": "posts/2021-01-24-op_lowering_pythorchXLA.html#go-deeper-with-the-lowering",
    "title": "Lowering SiLU in pytorch/XLA",
    "section": "2. Go deeper with the lowering",
    "text": "2. Go deeper with the lowering\nNow that you have a base go deeper and make your node appear.\nBecause when people is debugging the generated graph of tensor ops in XLA with the previous implementation it would be better if calling SiLU would generate a SiLU node and not x * sigmoid(x) as the previous step.\nSecond commit https://github.com/pytorch/xla/pull/2721/commits/c16fedbbee3662d3470629dc7fff51c63dd60855\nIt shows how to add an op that will be used as a node in the generated graph for the operation.\n\nIt adds the operation to ops.h/cpp\nIt converts from tensors to XLA tensors and back.\nIt reuses at this level the implementation of SiLU (which is valid because you have already named the node at this level) which is node.ReturnOp(xla_input * BuildSigmoid(xla_input), loctx) and provides a “name” for the node with GenericOp(OpKind(at::aten::silu), {input}, input.shape(), std::move(lower_fn)).\n\nMy first approach was to repeat all so I duplicated the sigmoid implemented in elementwise.h/cpp and used that but the review of the PR suggested that I can call sigmoid because the node was already a SiLU so it doesn’t matter if I reused what was already there at that moment. I corrected with an amended and just reused Sigmoid instead of my SiLU in elementwise, making the commit writes 2 less files than amended commit."
  },
  {
    "objectID": "posts/2021-01-24-op_lowering_pythorchXLA.html#the-backward-pass",
    "href": "posts/2021-01-24-op_lowering_pythorchXLA.html#the-backward-pass",
    "title": "Lowering SiLU in pytorch/XLA",
    "section": "The backward pass",
    "text": "The backward pass\nThis operation didn’t include a backward pass, you should implement it if the header contains the forward and the backward pass, this was more a _out operation that is also used for in place methods.\nNote: I haven’t seen how this dispatch works, but I guess works like simple inheritance, when aten_xla_type don’t provide the method then the ones from aten_xla_type_default are used (which is the CPU implementations and fallbacks). But see that the type AtenXlaType is not a subclass aten_xla_type”\n“Also note that aten_xla_type_default which is auto generated after build in some stage because it is not in repo and is ignored in .gitignore. So it should be other type of dynamic dispatching somewhere deep in the code."
  },
  {
    "objectID": "posts/2020-09-26-librosa-2015-presentation-updated-calls.html",
    "href": "posts/2020-09-26-librosa-2015-presentation-updated-calls.html",
    "title": "librosa 2015 presentation updated calls",
    "section": "",
    "text": "From https://www.youtube.com/watch?v=MhOdbtPhbLU in 2015\nI also copy & paste some captures (they look weird) to see how different it was 5 years a go.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn\n\nseaborn.set(style='ticks')\nfrom IPython.display import Audio\n\nimport numpy as np\nimport scipy\n\nimport mir_eval\n\nimport librosa\nimport librosa.display\n\n\ny,sr = librosa.load(librosa.util.example_audio_file())\n\n\n\n\n\n\n\nImportant\n\n\n\nI removed all the outputs of Audio(data=,rate=) because the notebook was more than 19Mb total\n\n\n\nAudio(data=y,rate=sr) # cleaned just to lower size of notebook (run again to see embeded player)\n\n\nwaveform\n\nlibrosa.display.waveplot(y, sr)\n\n<matplotlib.collections.PolyCollection at 0x7fcb930f68b0>\n\n\n\n\n\n\n\nspectrograms\n\nD = librosa.stft(y)\nD.shape\n\n(1025, 2647)\n\n\n\nlog_spectrogram = librosa.power_to_db(D**2, ref=np.max)\nlog_spectrogram.shape\n\n/home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/core/spectrum.py:1544: UserWarning: power_to_db was called on complex input so phase information will be discarded. To suppress this warning, call power_to_db(np.abs(D)**2) instead.\n  warnings.warn(\n\n\n(1025, 2647)\n\n\n\nlibrosa.display.specshow(log_spectrogram, x_axis='time', y_axis='linear')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fcb93009370>\n\n\n\n\n\n\nlibrosa.display.specshow(log_spectrogram, x_axis='time', y_axis='log')\nplt.colorbar()\n\n/home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The 'basey' parameter of __init__() has been renamed 'base' since Matplotlib 3.3; support for the old name will be dropped two minor releases later.\n  scaler(mode, **kwargs)\n/home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The 'linthreshy' parameter of __init__() has been renamed 'linthresh' since Matplotlib 3.3; support for the old name will be dropped two minor releases later.\n  scaler(mode, **kwargs)\n/home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The 'linscaley' parameter of __init__() has been renamed 'linscale' since Matplotlib 3.3; support for the old name will be dropped two minor releases later.\n  scaler(mode, **kwargs)\n\n\n<matplotlib.colorbar.Colorbar at 0x7fcb92f4e0a0>\n\n\n\n\n\n\n\nConstant q transform\ndirect log-frecuency analysis?\n\nC = librosa.cqt(y, sr)\nC.shape\n\n(84, 2647)\n\n\n\nlibrosa.display.specshow(librosa.amplitude_to_db(C**2), x_axis='time', y_axis='cqt_hz')\nplt.colorbar()\n\n/home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/core/spectrum.py:1641: UserWarning: amplitude_to_db was called on complex input so phase information will be discarded. To suppress this warning, call amplitude_to_db(np.abs(S)) instead.\n  warnings.warn(\n\n\n<matplotlib.colorbar.Colorbar at 0x7fcb92e887c0>\n\n\n\n\n\n\n\nlibrosa.display.specshow(librosa.amplitude_to_db(C**2, top_db=40), x_axis='time', y_axis='cqt_note')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fcb92d0f0d0>\n\n\n\n\n\n\n\n\nSpectral features\nSpectral features are often used to analyze harmony or timbre.\nUsually the product of a spectrogram and a filter bank.\n\n\npitch vs class\nCQT measures the energy in each pitch.\nChroma measures the energy in each pitch class.\n\nchroma = librosa.feature.chroma_cqt(C=C, sr=sr)\nchroma.shape\n\n(12, 2647)\n\n\n\nlibrosa.display.specshow(chroma, x_axis='time', y_axis='chroma')\nplt.colorbar()\n\n/home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:822: UserWarning: Trying to display complex-valued input. Showing magnitude instead.\n  warnings.warn(\n\n\n<matplotlib.colorbar.Colorbar at 0x7fcb906852b0>\n\n\n\n\n\n\nOther spectral features includes MEL spectra, MFCC and Tonnetz\n\nM = librosa.feature.melspectrogram(y=y, sr=sr)\nMFCC = librosa.feature.mfcc(y=y, sr=sr)\ntonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n\n\n\nAudio effects\n\ny_harmonic, y_percussive = librosa.effects.hpss(y)\n\n\nAudio(data=y, rate=sr)\n\n\nAudio(data=y_harmonic, rate=sr)\n\n\nAudio(data=y_percussive, rate=sr)\n\n\nplt.figure(figsize=(12,6))\nC_harmonic = librosa.cqt(y_harmonic, sr)\nC_perc = librosa.cqt(y_percussive, sr)\n\nplt.subplot(3,1,1), librosa.display.specshow(C**(1./3), y_axis='cqt_hz'), plt.colorbar()\nplt.subplot(3,1,2), librosa.display.specshow(C_harmonic**(1./3), y_axis='cqt_hz'), plt.colorbar()\nplt.subplot(3,1,3), librosa.display.specshow(C_perc**(1./3), y_axis='cqt_hz'), plt.colorbar()\n\n(<AxesSubplot:ylabel='Hz'>,\n <matplotlib.collections.QuadMesh at 0x7fcb92d88d90>,\n <matplotlib.colorbar.Colorbar at 0x7fcb92df3a00>)\n\n\n\n\n\n\n\nOnsets and beats\n\nonset_envelope = librosa.onset.onset_strength(y, sr)\n\n\nonsets = librosa.onset.onset_detect(onset_envelope=onset_envelope)\n\n\nplt.subplot(2,1,1)\nplt.plot(onset_envelope, label='Onset strength')\nplt.vlines(onsets, 0, onset_envelope.max(), color='r', alpha=0.25, label='onsets')\nplt.xticks([]), plt.yticks([])\nplt.legend(frameon=True)\nplt.axis('tight')\n\nplt.subplot(2,1,2)\nlibrosa.display.waveplot(y, sr)\n\n<matplotlib.collections.PolyCollection at 0x7fcb93b6a220>\n\n\n\n\n\nonset stregth is used to track beats and estimate tempo\n\ntempo, beats = librosa.beat.beat_track(onset_envelope=onset_envelope)\ntempo, beats\n\n(129.19921875,\n array([   4,   23,   43,   63,   83,  102,  122,  142,  162,  181,  202,\n         222,  242,  261,  281,  301,  321,  341,  361,  382,  401,  421,\n         441,  461,  480,  500,  520,  540,  560,  579,  600,  620,  639,\n         658,  678,  698,  718,  737,  757,  777,  798,  817,  837,  857,\n         877,  896,  916,  936,  957,  976,  996, 1016, 1036, 1055, 1075,\n        1095, 1116, 1135, 1155, 1175, 1195, 1214, 1234, 1254, 1275, 1294,\n        1314, 1334, 1354, 1373, 1393, 1413, 1434, 1453, 1473, 1493, 1513,\n        1532, 1552, 1572, 1593, 1612, 1632, 1652, 1672, 1691, 1712, 1732,\n        1752, 1771, 1791, 1811, 1831, 1850, 1870, 1890, 1911, 1931, 1951,\n        1971, 1990, 2010, 2030, 2050, 2070, 2090, 2110, 2130, 2149, 2169,\n        2189, 2209, 2229, 2249, 2269, 2288, 2308, 2328, 2348, 2368, 2388,\n        2408, 2428, 2448, 2467, 2487, 2507, 2527, 2547]))\n\n\n\nplt.plot(onset_envelope, label='Onset strength')\nplt.vlines(onsets, 0, onset_envelope.max(), color='r', alpha=0.25, label='onsets')\nplt.xticks([]), plt.yticks([])\nplt.legend(frameon=True)\nplt.axis('tight')\n\n(-132.3, 2778.3, -0.05, 1.05)\n\n\n\n\n\nbeat events are in frame indices\nWe can convert to time (in seconds), and sonify with mir_eval\n\nbeat_times = librosa.frames_to_time(beats)\nbeat_times\n\narray([ 0.09287982,  0.53405896,  0.99845805,  1.46285714,  1.92725624,\n        2.36843537,  2.83283447,  3.29723356,  3.76163265,  4.20281179,\n        4.69043084,  5.15482993,  5.61922902,  6.06040816,  6.52480726,\n        6.98920635,  7.45360544,  7.91800454,  8.38240363,  8.87002268,\n        9.31120181,  9.77560091, 10.24      , 10.70439909, 11.14557823,\n       11.60997732, 12.07437642, 12.53877551, 13.0031746 , 13.44435374,\n       13.93197279, 14.39637188, 14.83755102, 15.27873016, 15.74312925,\n       16.20752834, 16.67192744, 17.11310658, 17.57750567, 18.04190476,\n       18.52952381, 18.97070295, 19.43510204, 19.89950113, 20.36390023,\n       20.80507937, 21.26947846, 21.73387755, 22.2214966 , 22.66267574,\n       23.12707483, 23.59147392, 24.05587302, 24.49705215, 24.96145125,\n       25.42585034, 25.91346939, 26.35464853, 26.81904762, 27.28344671,\n       27.7478458 , 28.18902494, 28.65342404, 29.11782313, 29.60544218,\n       30.04662132, 30.51102041, 30.9754195 , 31.43981859, 31.88099773,\n       32.34539683, 32.80979592, 33.29741497, 33.7385941 , 34.2029932 ,\n       34.66739229, 35.13179138, 35.57297052, 36.03736961, 36.50176871,\n       36.98938776, 37.43056689, 37.89496599, 38.35936508, 38.82376417,\n       39.26494331, 39.75256236, 40.21696145, 40.68136054, 41.12253968,\n       41.58693878, 42.05133787, 42.51573696, 42.9569161 , 43.42131519,\n       43.88571429, 44.37333333, 44.83773243, 45.30213152, 45.76653061,\n       46.20770975, 46.67210884, 47.13650794, 47.60090703, 48.06530612,\n       48.52970522, 48.99410431, 49.4585034 , 49.89968254, 50.36408163,\n       50.82848073, 51.29287982, 51.75727891, 52.221678  , 52.6860771 ,\n       53.12725624, 53.59165533, 54.05605442, 54.52045351, 54.98485261,\n       55.4492517 , 55.91365079, 56.37804989, 56.84244898, 57.28362812,\n       57.74802721, 58.2124263 , 58.6768254 , 59.14122449])\n\n\n\ny_click = mir_eval.sonify.clicks(beat_times, sr, length=len(y))\nAudio(data=y+y_click, rate=sr)\n\n\n\nTemporal structure\n\nc_sync = librosa.util.sync(chroma, beats, aggregate=np.median)\nc_sync.shape\n\n(12, 130)\n\n\n\nlibrosa.display.specshow(c_sync, y_axis='chroma')\nplt.colorbar()\n\n/home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:822: UserWarning: Trying to display complex-valued input. Showing magnitude instead.\n  warnings.warn(\n\n\n<matplotlib.colorbar.Colorbar at 0x7fcb904d6100>\n\n\n\n\n\n\n\nhistory embedding can add context\n\nchroma_stack = librosa.feature.stack_memory(c_sync, n_steps=3, mode='edge')\nchroma_stack.shape\n\n(36, 130)\n\n\n\nlibrosa.display.specshow(chroma_stack, y_axis='chroma')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fcb903be0d0>\n\n\n\n\n\n\n\nrecurrence plots show nearest neighbor linkage for each frame.\nChroma recurrence can encode harmonic repetitions\n\n# cant run this cell, it eats up 32 Gb physical memory + 37 Gb of swap on my Linux\n# R = librosa.segment.recurrence_matrix(y, sym=True)\n\n\n#R = librosa.segment.recurrence_matrix(y, sym=True)\n#R = librosa.segment.recurrence_matrix(chroma_stack, sym=True)\n# diagonal lines indicate repeated progressions\n# librosa.display.specshow(R, aspect='equal')\n# post processing R can reveal structural components, metrical structure, etc\n\nHow to plot the different Rs above?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Curvando la línea para llevar un fraude\n\n\n\n\n\nAnalisar el prep para mostrar el fraude desde la comidad de la casa\n\n\n\n\n\n\nJun 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\nDos and don’ts with invitation to make study groups\n\n\n\n\n\nmy first ever study group experience and the invitation to do it yourself! and feel the edge!\n\n\n\n\n\n\nApr 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLowering SiLU in pytorch/XLA\n\n\n\n\n\n\n\nxla\n\n\n\n\nLowering SiLU operation in Pytorch/XLA\n\n\n\n\n\n\nJan 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAudio y auto estudio interview\n\n\n\n\n\n\n\nfastchai\n\n\nfastai\n\n\n\n\nAudio y estudio autodidacta\n\n\n\n\n\n\nJan 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n¿Cómo reconocer las buenas ideas de investigación que valen tu tiempo y esfuerzo?\n\n\n\n\n\n\n\nnote\n\n\n\n\nCómo reconocer las buenas ideas de investigación\n\n\n\n\n\n\nJan 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding nemo a bug journey\n\n\n\n\n\nfollow up from having locally installed xla\n\n\n\n\n\n\nDec 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\nCompiling xla locally\n\n\n\n\n\ntools and env vars needed to compile xla locally with GPU support\n\n\n\n\n\n\nNov 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrosa 2015 presentation updated calls\n\n\n\n\n\nA tutorial of fastpages for Jupyter notebooks.\n\n\n\n\n\n\nSep 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReconociendo dígitos en fastai2\n\n\n\n\n\nComo reconocer los dígitos de la competición https://www.kaggle.com/c/digit-recognizer\n\n\n\n\n\n\nMay 26, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "tyoc213 blog’s",
    "section": "",
    "text": "I’m a developer that has been exploring all types of applications from: native mobile on C++/Android/iOS and crossplatform to backends in different languages doing custom apps, financial apps or core banking services.\nI have always liked:\n\nOS\nLanguage R&D\nGames\nArtifial intelligence\nBioinformatics\n\nAnd in this blog I rant or write on Español o English."
  }
]